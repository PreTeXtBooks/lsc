<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch-why-statistics" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Why Do We Learn Statistics?</title>

  <introduction>
    <p>
      To the surprise of many students, statistics is a fairly significant part of a psychological
      education. To the surprise of no-one, statistics is very rarely the <em>favourite</em> part
      of one's psychological education. Not surprisingly, there's a pretty large proportion of the
      student base that isn't happy about the fact that psychology has so much statistics in it.
      A big part of this issue at hand relates to the very idea of statistics.
    </p>

    <blockquote>
      <p>
        <em>Why do you do statistics? Why don't scientists just use <alert>common sense?</alert></em>
      </p>
    </blockquote>

    <p>
      Humans are susceptible to all kinds of biases, temptations and frailties, and much of
      statistics is basically a safeguard. Using <q>common sense</q> to evaluate evidence means
      trusting gut instincts, relying on verbal arguments and on using the raw power of human
      reason to come up with the right answer, which is not scientific at all.
    </p>
  </introduction>

  <section xml:id="sec-belief-bias">
    <title>The Curse of Belief Bias</title>

    <p>
      Psychologists have shown over the years is that we really do find it hard to be neutral,
      to evaluate evidence impartially and without being swayed by pre-existing biases. A good
      example of this is the <term>belief bias effect</term> in logical reasoning: if you ask
      people to decide whether a particular argument is logically valid (i.e., conclusion would
      be true if the premises were true), we tend to be influenced by the believability of the
      conclusion, even when we shouldn't. For instance, here's a valid argument where the
      conclusion is believable:
    </p>

    <blockquote>
      <p>No cigarettes are inexpensive (Premise 1)</p>
      <p>Some addictive things are inexpensive (Premise 2)</p>
      <p>Therefore, some addictive things are not cigarettes (Conclusion)</p>
    </blockquote>

    <p>And here's a valid argument where the conclusion is not believable:</p>

    <blockquote>
      <p>No addictive things are inexpensive (Premise 1)</p>
      <p>Some cigarettes are inexpensive (Premise 2)</p>
      <p>Therefore, some cigarettes are not addictive (Conclusion)</p>
    </blockquote>

    <p>
      The logical <em>structure</em> of argument #2 is identical to the structure of argument #1,
      and they're both valid. However, in the second argument, there are good reasons to think
      that premise 1 is incorrect, and as a result it's probably the case that the conclusion is
      also incorrect. But that's entirely irrelevant to the topic at hand: an argument is
      deductively valid if the conclusion is a logical consequence of the premises. That is, a
      valid argument doesn't have to involve true statements.
    </p>

    <p>On the other hand, here's an invalid argument that has a believable conclusion:</p>

    <blockquote>
      <p>No addictive things are inexpensive (Premise 1)</p>
      <p>Some cigarettes are inexpensive (Premise 2)</p>
      <p>Therefore, some addictive things are not cigarettes (Conclusion)</p>
    </blockquote>

    <p>And finally, an invalid argument with an unbelievable conclusion:</p>

    <blockquote>
      <p>No cigarettes are inexpensive (Premise 1)</p>
      <p>Some addictive things are inexpensive (Premise 2)</p>
      <p>Therefore, some cigarettes are not addictive (Conclusion)</p>
    </blockquote>

    <p>
      Now, suppose that people really are perfectly able to set aside their pre-existing biases
      about what is true and what isn't, and purely evaluate an argument on its logical merits.
      We'd expect 100% of people to say that the valid arguments are valid, and 0% of people to
      say that the invalid arguments are valid. So if you ran an experiment looking at this,
      you'd expect to see data like this:
    </p>

    <table xml:id="table-conclusion1">
      <title/>
      <tabular halign="center">
        <row header="yes">
          <cell halign="left"/>
          <cell>conclusion feels true</cell>
          <cell>conclusion feels false</cell>
        </row>
        <row>
          <cell halign="left">argument is valid</cell>
          <cell>100% say valid</cell>
          <cell>100% say valid</cell>
        </row>
        <row>
          <cell halign="left">argument is invalid</cell>
          <cell>0% say valid</cell>
          <cell>0% say valid</cell>
        </row>
      </tabular>
    </table>

    <p>
      If the psychological data looked like this (or even a good approximation to this), we
      might feel safe in just trusting our gut instincts. That is, it'd be perfectly okay just
      to let scientists evaluate data based on their common sense, and not bother with all this
      murky statistics stuff.
    </p>

    <p>
      In a classic study, Evans, Barston, and Pollard <xref ref="Evans1983"/> ran an experiment
      looking at exactly this. What they found is that when pre-existing biases (i.e., beliefs)
      were in agreement with the structure of the data, everything went the way you'd hope:
    </p>

    <table xml:id="table-conclusion2">
      <title/>
      <tabular halign="center">
        <row header="yes">
          <cell halign="left"/>
          <cell>conclusion feels true</cell>
          <cell>conclusion feels false</cell>
        </row>
        <row>
          <cell halign="left">argument is valid</cell>
          <cell>92% say valid</cell>
          <cell/>
        </row>
        <row>
          <cell halign="left">argument is invalid</cell>
          <cell/>
          <cell>8% say valid</cell>
        </row>
      </tabular>
    </table>

    <p>
      Not perfect, but that's pretty good. But look what happens when our intuitive feelings
      about the truth of the conclusion run against the logical structure of the argument:
    </p>

    <table xml:id="table-conclusion3">
      <title/>
      <tabular halign="center">
        <row header="yes">
          <cell halign="left"/>
          <cell>conclusion feels true</cell>
          <cell>conclusion feels false</cell>
        </row>
        <row>
          <cell halign="left">argument is valid</cell>
          <cell>92% say valid</cell>
          <cell><alert>46% say valid</alert></cell>
        </row>
        <row>
          <cell halign="left">argument is invalid</cell>
          <cell><alert>92% say valid</alert></cell>
          <cell>8% say valid</cell>
        </row>
      </tabular>
    </table>

    <p>
      Apparently, when people are presented with a strong argument that contradicts our
      pre-existing beliefs, we find it pretty hard to even perceive it to be a strong argument
      (people only did so 46% of the time). Even worse, when people are presented with a weak
      argument that agrees with our pre-existing biases, almost no-one can see that the argument
      is weak.
    </p>

    <p>
      It's just <em>too easy</em> for us to <q>believe what we want to believe</q>; so if we
      want to <q>believe in the data</q> instead, we're going to need a bit of help to keep our
      personal biases under control. That's what statistics does: it helps keep us honest.
    </p>
  </section>

  <section xml:id="sec-simpsons-paradox">
    <title>The Simpson's Paradox</title>

    <p>
      In 1973, the University of California, Berkeley, had some worries about the gender breakdown
      of student admissions into postgraduate courses. Given that there were nearly 13,000
      applicants, a difference of 9 percentage points in admission rates between males and females
      is just way too big to be a coincidence.
    </p>

    <table xml:id="table-applicants">
      <title/>
      <tabular halign="center">
        <row header="yes">
          <cell halign="left"/>
          <cell>Number of applicants</cell>
          <cell>Percent admitted</cell>
        </row>
        <row>
          <cell halign="left">Males</cell>
          <cell>8442</cell>
          <cell>46%</cell>
        </row>
        <row>
          <cell halign="left">Females</cell>
          <cell>4321</cell>
          <cell>35%</cell>
        </row>
      </tabular>
    </table>

    <p>
      When people started looking more carefully at the admissions data
      <xref ref="Bickel1975"/> on a department by department basis, it turned out that most of
      the departments actually had a slightly <em>higher</em> success rate for female applicants
      than for male applicants.
    </p>

    <table xml:id="table-simpsontable">
      <title/>
      <tabular halign="center">
        <row header="yes">
          <cell/>
          <cell colspan="2">Male</cell>
          <cell colspan="2">Female</cell>
        </row>
        <row header="yes">
          <cell halign="left">Department</cell>
          <cell>Applicants</cell>
          <cell>Admitted</cell>
          <cell>Applicants</cell>
          <cell>Admitted</cell>
        </row>
        <row>
          <cell halign="left">A</cell>
          <cell>825</cell>
          <cell>62%</cell>
          <cell>108</cell>
          <cell>82%</cell>
        </row>
        <row>
          <cell halign="left">B</cell>
          <cell>560</cell>
          <cell>63%</cell>
          <cell>25</cell>
          <cell>68%</cell>
        </row>
        <row>
          <cell halign="left">C</cell>
          <cell>325</cell>
          <cell>37%</cell>
          <cell>593</cell>
          <cell>34%</cell>
        </row>
        <row>
          <cell halign="left">D</cell>
          <cell>417</cell>
          <cell>33%</cell>
          <cell>375</cell>
          <cell>35%</cell>
        </row>
        <row>
          <cell halign="left">E</cell>
          <cell>191</cell>
          <cell>28%</cell>
          <cell>393</cell>
          <cell>24%</cell>
        </row>
        <row>
          <cell halign="left">F</cell>
          <cell>272</cell>
          <cell>6%</cell>
          <cell>341</cell>
          <cell>7%</cell>
        </row>
      </tabular>
    </table>

    <p>
      Yet the overall rate of admission across the university for females was <em>lower</em> than
      for males. How can both of these statements be true at the same time?
    </p>

    <p>
      Firstly, notice that the departments are <em>not</em> equal to one another in terms of
      their admission percentages: some departments (e.g., engineering, chemistry) tended to
      admit a high percentage of the qualified applicants, whereas others (e.g., English) tended
      to reject most of the candidates, even if they were high quality. So, among the six
      departments shown above, notice that department A is the most generous, followed by B, C,
      D, E and F in that order. Next, notice that males and females tended to apply to different
      departments. If we rank the departments in terms of the total number of male applicants, we
      get <alert>A</alert>&gt;<alert>B</alert>&gt;D&gt;C&gt;F&gt;E (the
      <q>easy</q> departments are in bold). On the whole, males tended to apply to the departments
      that had high admission rates. Now compare this to how the female applicants distributed
      themselves. Ranking the departments in terms of the total number of female applicants
      produces a quite different ordering
      C&gt;E&gt;D&gt;F&gt;<alert>A</alert>&gt;<alert>B</alert>.
    </p>

    <p>
      In other words, what these data seem to be suggesting is that the female applicants tended
      to apply to <q>harder</q> departments. And in fact, if we look at <xref ref="fig-berkeley"/>,
      we see that this trend is systematic, and quite striking. This effect is known as Simpson's
      paradox. It's not common, but it does happen in real life, and most people are very surprised
      by it when they first encounter it, and many people refuse to even believe that it's real.
      It is very real.
    </p>

    <figure xml:id="fig-berkeley">
      <caption>
        The Berkeley 1973 college admission rate for the 85 departments that had at least one
        female applicant based on Bickel et al. <xref ref="Bickel1975"/>. Circles plot departments
        with more than 40 applicants; the area of the circle is proportional to the total number
        of applicants. The crosses plot department with fewer than 40 applicants.
      </caption>
      <image source="berkeley.png" width="70%">
        <description>
          A scatter plot of the Berkeley 1973 college admissions data showing the relationship
          between the percentage of female applicants and the admission rate across 85 departments.
        </description>
      </image>
    </figure>

    <p>
      When doing research, there are <em>lots</em> of subtle, counterintuitive traps lying in
      wait for the unwary. Truth is sometimes cunningly hidden in the nooks and crannies of
      complicated data. Statistics only solves <em>part</em> of the problem.
    </p>

    <p>
      Remember that we started, we looked at the <q>aggregated</q> data, and it did seem like
      the university was discriminating against women, but when we <q>disaggregated</q> and
      looked at the individual behaviour of all the departments, it turned out that the actual
      departments were, if anything, slightly biased in favour of women. The gender bias in total
      admissions was caused by the fact that women tended to self-select for harder departments.
      If you're interested in the overall structural effects of subtle gender biases, then you
      probably want to look at <em>both</em> the aggregated and disaggregated data. If you're
      interested in the decision making process at Berkeley itself then you're probably only
      interested in the disaggregated data.
    </p>

    <p>
      In short there are a lot of critical questions that you can't answer with statistics, but
      the answers to those questions will have a huge impact on how you analyse and interpret
      data. And this is the reason why you should always think of statistics as a <em>tool</em>
      to help you learn about your data, no more and no less. It's a powerful tool to that end,
      but there's no substitute for careful thought.
    </p>
  </section>

  <section xml:id="sec-statistics-in-psychology">
    <title>Statistics in Psychology</title>

    <p>
      We hope that the discussion above helped explain why science in general is so focused on
      statistics. But we're guessing that you have a lot more questions about what role statistics
      plays in psychology, and specifically why psychology classes always devote so many lectures
      to stats. So here's an attempt to answer a few of them...
    </p>

    <paragraphs xml:id="par-why-so-much-stats">
      <title>Why does psychology have so much statistics?</title>

      <p>
        The most important reason is that psychology is a statistical science. There's a saying
        used sometimes in physics, to the effect that <q>if your experiment needs statistics, you
        should have done a better experiment</q>.
      </p>

      <p>
        The <q>things</q> that we study are <em>people</em>. Real, complicated, gloriously messy
        people. The <q>things</q> of physics include object like electrons, and while there are
        all sorts of complexities that arise in physics, electrons don't have minds of their own.
        They don't have opinions, they don't differ from each other in weird and arbitrary ways,
        they don't get bored in the middle of an experiment, and they don't get angry at the
        experimenter and then deliberately try to sabotage the data set.<fn>Some may argue that
        natural science experiments still struggle with data quality, noise etc. But there's no
        evidence of an electron producing response bias in the lab due to over- or undercompliance,
        which is our point.</fn>
      </p>

      <p>
        We teach statistics to you as psychologists because you need to be better at stats than
        physicists. They have the luxury of being able to say that because their objects of study
        are simple in comparison to the vast mess that confronts social scientists.
      </p>
    </paragraphs>

    <paragraphs xml:id="par-someone-else-stats">
      <title>Can't someone else do the statistics?</title>

      <p>
        To some extent, but not completely. It's true that you don't need to become a fully
        trained statistician just to do psychology, but you do need to reach a certain level of
        statistical competence. There's three reasons that every psychological researcher ought
        to be able to do basic statistics:
      </p>

      <ul>
        <li>
          <p>
            Firstly, there's the fundamental reason: statistics is deeply intertwined with research
            design. If you want to be good at designing psychological studies, you need to at least
            understand the basics of stats.
          </p>
        </li>
        <li>
          <p>
            Secondly, if you want to be good at the psychological side of the research, then you
            need to be able to understand the psychological literature, right? But almost every
            paper in the psychological literature reports the results of statistical analyses. So
            if you really want to understand the psychology, you need to be able to understand what
            other people did with their data. And that means understanding a certain amount of
            statistics.
          </p>
        </li>
        <li>
          <p>
            Thirdly, there's a big practical problem with being dependent on other people to do
            all your statistics: statistical analysis is <em>expensive</em>. In almost any real
            life situation where you want to do psychological research, the cruel facts will be
            that you don't have enough money to afford a statistician. So the economics of the
            situation mean that you have to be self-sufficient.
          </p>
        </li>
      </ul>

      <p>
        Note that a lot of these reasons generalise beyond researchers. If you want to be a
        practicing psychologist and stay on top of the field, it helps to be able to read the
        scientific literature, which relies pretty heavily on statistics.
      </p>
    </paragraphs>

    <paragraphs xml:id="par-dont-care-about-jobs">
      <title>I don't care about jobs, research, or clinical work. Do I need statistics?</title>

      <p>
        Statistics should matter to you in the same way that statistics should matter to
        <em>everyone</em>: we live in the 21st century, and data are <em>everywhere</em>.
        Frankly, given the world in which we live these days, a basic knowledge of statistics is
        pretty damn close to a survival tool! Which is the topic of the next section...
      </p>
    </paragraphs>
  </section>

  <section xml:id="sec-more-than-statistics">
    <title>There's More to Research Methods Than Statistics</title>

    <p>
      Most research methods courses will cover a lot of topics that relate much more to the
      pragmatics of research design, and in particular the issues that you encounter when trying
      to do research with humans. Most student fears relate to the statistics part of the course.
      Hopefully you are convinced that statistics matters, and more importantly, that it's not to
      be feared.
    </p>

    <p>
      Introductory classes focus a lot on the statistics because you almost always find yourself
      needing statistics before you need the other research methods training. Why? Because almost
      all of your assignments in other classes will rely on statistical training, to a much
      greater extent than they rely on other methodological tools. It's not common for
      undergraduate assignments to require you to design your own study from the ground up (in
      which case you would need to know a lot about research design), but it is common for
      assignments to ask you to analyse and interpret data that were collected in a study that
      someone else designed (in which case you need statistics). In that sense, from the
      perspective of allowing you to do well in all your other classes, the statistics is more
      urgent.
    </p>

    <p>
      But note that <q>urgent</q> is different from <q>important</q> -- they both matter. We
      really do want to stress that research design is just as important as data analysis, and
      this book does spend a fair amount of time on it. However, while statistics has a kind of
      universality, and provides a set of core tools that are useful for most types of
      psychological research, the research methods side isn't quite so universal. There are some
      general principles that everyone should think about, but a lot of research design is very
      idiosyncratic, and is specific to the area of research that you want to engage in. To the
      extent that it's the details that matter, those details don't usually show up in an
      introductory stats and research methods class.
    </p>
  </section>

</chapter>
