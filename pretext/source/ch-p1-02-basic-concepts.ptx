<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch-basic-concepts" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Basic Concepts</title>

  <introduction>
    <p>
      In this chapter, we'll discuss basic concepts and terminology related to measurement and
      study design. Since this book focuses on data analysis, it won't give you enough information
      to design studies of your own. To do that, you'll need to take a course in research design
      and methods. There are a couple of good books on the topic.
    </p>
    <p>
      This chapter relies on Howitt and Cramer <xref ref="howittUnderstandingStatisticsPsychology2020"/>
      and Stevens <xref ref="Stevens1946"/> for discussing scales of measurement, and on
      Boncz <xref ref="bonczResearchMethodologyBasics2015"/>,
      Michalos <xref ref="michalosEncyclopediaQualityLife2014"/>, and
      Marks and Yardley <xref ref="marksResearchMethodsClinical2004"/> for discussing reliability
      and validity.
    </p>
  </introduction>

  <section xml:id="sec-measurement">
    <title>Introduction to Psychological Measurement</title>

    <p>
      First, data collection can be thought of as a kind of <alert>measurement</alert>. That is,
      what we're trying to do here is measure something about human behaviour or the human mind.
      What do we mean by <q>measurement</q>?
    </p>

    <p>
      Measurement as a concept comes down to finding some way of assigning numbers, labels, or
      other well-defined descriptions to phenomena. So, any of the following would count as a
      psychological measurement:
    </p>

    <ul>
      <li><p>Danielle's <alert>age</alert> is <em>33 years</em>.</p></li>
      <li><p>She <em>does not</em> <alert>like anchovies</alert>.</p></li>
      <li><p>Her <alert>chromosomal gender</alert> is <em>male</em>.</p></li>
      <li><p>Her <alert>self-identified gender</alert> is <em>female</em>.</p></li>
    </ul>

    <p>
      In the short list above, the <alert>bolded part</alert> is <q>the thing to be measured</q>,
      and the <em>italicised part</em> is <q>the measurement itself</q>. We can expand on this a
      little bit by thinking about the set of possible measurements that could have arisen in each
      case:
    </p>

    <ul>
      <li>
        <p>
          <alert>Age</alert> (in years) could have been <em>1, 2, 3 ... years</em>, etc. The
          upper bound on what the age could be is a bit fuzzy, but in practice, you'd be safe in
          saying that the largest possible age is <em>150</em> since no human has ever lived that
          long. And age doesn't really have a true zero, but that is not important just yet.
        </p>
      </li>
      <li>
        <p>
          When asked if someone <alert>likes anchovies</alert>, they might say <em>I do</em>,
          <em>I do not</em>, <em>I have no opinion</em>, or <em>I sometimes do</em>.
        </p>
      </li>
      <li>
        <p>
          <alert>Chromosomal gender</alert> is almost certainly going to be
          <em>male (XY)</em> or <em>female (XX)</em>, but there are a few other possibilities:
          with <em>Klinefelter's syndrome (XXY)</em>, it is more similar to male than to female.
          And there are other possibilities, too.
        </p>
      </li>
      <li>
        <p>
          <alert>Self-identified gender</alert> is also very likely to be <em>male</em> or
          <em>female</em>, <em>transgender</em>, <em>nonbinary</em>, <em>queer</em> etc.
        </p>
      </li>
    </ul>

    <p>
      As you can see, for some things, like age, it seems pretty apparent what the set of possible
      measurements should be, whereas, for other things, it gets a bit tricky. But even regarding
      someone's age, it's a bit more subtle. For instance, if you're a developmental psychologist,
      measuring in <em>years</em> is way too crude, and so you often measure age in
      <em>years and months</em> (if a child is 2 years and 11 months, this is usually written as
      <q>2;11</q>). If you're interested in newborns, you might want to measure age in
      <em>days since birth</em>, or maybe even <em>hours since birth</em>.
    </p>

    <p>
      Looking at this a bit more closely, you might also realise that the concept of <q>age</q>
      isn't all that precise. Generally, when we say <q>age</q>, we implicitly mean <q>the length
      of time since birth</q>. But that's not always the right way to do it.
    </p>

    <p>
      Suppose you're interested in how newborn babies control their eye movements. If Baby Alice
      is born 3 weeks premature and Baby Bianca is born 1 week late, would it really make sense
      to say that they are the <q>same age</q> if we encountered them <q>2 hours after birth</q>?
      In a sense, yes. By social convention, we use birth as our reference point for talking about
      age in everyday life since it defines the amount of time the person has been operating as an
      independent entity in the world. But from a scientific perspective, that's not the only
      thing we care about. You might want to measure age from conception, and voil√†, you're already
      thinking about all the potential problems with analysis and comparisons. When we think about
      the biology of human beings, it's often helpful to think of ourselves as organisms that have
      been growing and maturing since conception. From that perspective, Alice and Bianca aren't
      the same age at all. So you might want to define the concept of <q>age</q> in two different
      ways: the length of time since conception and the length of time since birth. It won't make
      much difference when dealing with adults, but when dealing with newborns, it might. In other
      words, how you specify the allowable measurement values is important.
    </p>

    <p>
      Still, there's the question of methodology. What specific measurement <alert>method</alert>
      will you use to find out someone's age? There are several options:
    </p>

    <ul>
      <li>
        <p>
          You could just ask people, <q>how old are you?</q> The method of <em>self-report</em>
          is fast, cheap and easy, but it only works with people old enough to understand the
          question, and some people lie about their age.
        </p>
      </li>
      <li>
        <p>
          You could ask an authority (e.g. a parent), <q>how old is your child?</q> This method
          is fast, and it's not all that hard when dealing with kids since the parent is almost
          always around. It doesn't work as well if you want to know <q>age since conception</q>
          since a lot of parents can't say for sure when conception took place. You might need a
          different authority (e.g. an obstetrician).
        </p>
      </li>
      <li>
        <p>
          You could look up official records, like birth certificates. This is time-consuming and
          annoying, but it has its uses (e.g. if the person is now dead).
        </p>
      </li>
    </ul>

    <p>
      All of the ideas discussed in the previous section relate to the concept of
      <alert>operationalisation</alert>. To be a bit more precise about the idea,
      operationalisation is the process by which we take a meaningful but somewhat vague concept
      and turn it into an accurate measurement. The method of operationalisation can involve
      several different things:
    </p>

    <ul>
      <li>
        <p>
          For instance, does <q>age</q> mean <q>time since birth</q> or <q>time since
          conception</q> in the context of your research?
        </p>
      </li>
      <li>
        <p>
          Will you use self-report to measure age, ask a parent, or look up an official record?
          If you're using self-report, how will you phrase the question?
        </p>
      </li>
      <li>
        <p>
          Defining the set of allowable values that the measurement can take. Note that these
          values don't always have to be numerical, though they often are. When measuring age,
          the values are numerical, but we still need to think carefully about what numbers are
          allowed. Do we want age in years, years and months, days, or hours? The values aren't
          numerical for other types of measurements (e.g. gender). But, as before, we need to
          consider what values are allowed. If we're asking people to self-report their gender,
          what options do we allow them to choose from? Is it enough to allow only <q>male</q> or
          <q>female</q>? Do you need an <q>other</q> option? Or should we not give people any
          specific options and let them answer in their own words? And if you open up the set of
          possible values to include all verbal responses, how will you interpret their answers?
        </p>
      </li>
    </ul>

    <p>
      Operationalisation is tricky, and there's no <q>one, true way</q> to do it. How you
      operationalise the informal concept of <q>age</q> or <q>gender</q> into a formal measurement
      depends on what you need to use the measurement for. You'll often find that the community of
      scientists who work in your area have some well-established ideas for how to go about it.
      Tip: when working on your dissertation, consult your supervisor and the literature to see
      what the community has settled on.
    </p>

    <p>
      In other words, operationalisation needs to be thought through case-by-case. Nevertheless,
      while there are a lot of issues that are specific to each individual research project, there
      are some aspects to it that are pretty general.
    </p>

    <p>
      Let's take a moment to clear up our terminology and, in the process, introduce one more
      term. Here are four different things that are closely related to each other:
    </p>

    <ul>
      <li>
        <p>
          <em>A clear construct</em>. This is the concept that you're trying to measure, like
          <q>age</q>, <q>gender</q>, or an <q>opinion</q>.
        </p>
      </li>
      <li>
        <p>
          <em>A measure</em>. The measure refers to the method or instrument used to make your
          observations. A question in a survey, a behavioural observation or a brain scan could
          all count as a measure.
        </p>
      </li>
      <li>
        <p>
          <em>Operationalisation</em>. This is the logical connection between the measure and the
          theoretical construct: a definition of how you assign value to the concept (calculation,
          range, level of measurement etc.).
        </p>
      </li>
      <li>
        <p>
          <em><alert>A variable</alert></em>. Finally, a new term. A variable is the outcome of
          the measurement process.
        </p>
      </li>
    </ul>

    <example xml:id="ex-operationalisation">
      <title>Method</title>
      <p>Here's an example:</p>
      <ul>
        <li>
          <p>
            Construct: attitude towards Prince Harry the Duke of Sussex (the underlying concept)
          </p>
        </li>
        <li>
          <p>
            Measure: 5-point Likert scale in a self-report survey (instrument)
          </p>
        </li>
        <li>
          <p>
            Operationalisation: survey a sample of UK online newsreaders and ask them to rate
            their level of agreement on a 5-point Likert scale with the following statement:
            <q>I personally find Prince Harry the Duke of Sussex an agreeable person</q><fn>Is
            this most relevant sentence that accurately represents the concept you want to
            measure?</fn>.
          </p>
        </li>
        <li>
          <p>
            Variable: the numerical value of the response to the question,
            e.g., 5: <q>strongly agree</q>...
          </p>
        </li>
      </ul>
    </example>
  </section>

  <section xml:id="sec-scales">
    <title>Measurement Levels</title>

    <p>
      As the previous section indicates, the outcome of a psychological measurement is called a
      variable. But not all variables are of the same qualitative type, and it's handy to
      understand what types there are. A very useful concept for distinguishing between different
      types of variables is what's known as <alert>scales of measurement</alert>, or in CogStat
      terminology, <alert>measurement levels</alert>.
    </p>

    <subsection xml:id="subsec-nominal">
      <title>Nominal Categories</title>

      <definition xml:id="def-nominal">
        <title>Nominal scale variable</title>
        <statement>
          <p>
            A <alert>nominal scale variable</alert> (also referred to as a
            <alert>categorical variable</alert>) is the results of a <em>qualitative</em>
            measurement, where the number of possible values are limited and fixed; they represent
            a name, a label, a classification, or non-overlapping categories; and there is no
            order, hierarchy or ranking between the categories. E.g. eye colour, place of
            residence, gender.
          </p>
        </statement>
      </definition>

      <p>
        For these kinds of variables, it doesn't make any sense to say that one of them is
        <q>bigger</q> or <q>better</q> than any other one, and it doesn't make any sense to
        average them. The classic example for this is <q>eye colour</q>. Eyes can be blue, green
        and brown, among other possibilities, but none of them is any <q>better</q> than any other
        one. As a result, it would feel bizarre to talk about an <q>average eye colour</q>.
        Similarly, gender is nominal too: <em>male</em> isn't better or worse than <em>female</em>,
        neither does it make sense to try to talk about an <q>average gender</q>. In short, nominal
        scale variables are those for which the only thing you can say about the different
        possibilities is that they are different. That's it.
      </p>

      <p>
        Note that sometimes nominal variables will have numbers coded to them usually for technical
        reasons (e.g., in survey data outputs). E.g., 1: male, 2: female, 3: nonbinary... This is
        a common practice, but the variable is still nominal: the numbers are just a way of
        representing the categories, and you should never, ever, ever (!) analyse them as if they
        were numerical/score measurements.
      </p>

      <example xml:id="ex-nominal">
        <title>Nominal scale variable example</title>
        <p>
          Suppose we were researching how people commute to and from work. One variable we would
          have to measure would be what kind of transportation people use to get to work. This
          <q>transport type</q> variable could have quite a few possible values, including:
          <q>train</q>, <q>bus</q>, <q>car</q>, <q>bicycle</q>, etc. For now, let's suppose that
          these four are the only possibilities, and suppose that when we ask 100 people how they
          got to work today, we get this:
        </p>

        <table xml:id="table-transportation1">
          <title/>
          <tabular halign="center">
            <row header="yes">
              <cell halign="left">Transportation</cell>
              <cell>Number of people</cell>
            </row>
            <row>
              <cell halign="left">Train</cell>
              <cell>12</cell>
            </row>
            <row>
              <cell halign="left">Bus</cell>
              <cell>30</cell>
            </row>
            <row>
              <cell halign="left">Car</cell>
              <cell>48</cell>
            </row>
            <row>
              <cell halign="left">Bicycle</cell>
              <cell>10</cell>
            </row>
          </tabular>
        </table>

        <p>
          So, what's the <em>average transportation type</em>? Obviously, the answer here is that
          there isn't one. You can say that travel by car is the <em>most popular method</em>, and
          travel by train is the <em>least popular method</em>, but that's about all. That is
          based on the <em>frequency</em> of the occurrence (i.e., count). Similarly, notice that
          the order in which the options are listed isn't very exciting. We could have chosen to
          display the data like this:
        </p>

        <table xml:id="table-transportation2">
          <title/>
          <tabular halign="center">
            <row header="yes">
              <cell halign="left">Transportation</cell>
              <cell>Number of people</cell>
            </row>
            <row>
              <cell halign="left">Car</cell>
              <cell>48</cell>
            </row>
            <row>
              <cell halign="left">Train</cell>
              <cell>12</cell>
            </row>
            <row>
              <cell halign="left">Bicycle</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell halign="left">Bus</cell>
              <cell>30</cell>
            </row>
          </tabular>
        </table>

        <p>
          -- and nothing really changes.
        </p>
      </example>
    </subsection>

    <subsection xml:id="subsec-ordinal">
      <title>Ordinal Scale and Rank</title>

      <definition xml:id="def-ordinal">
        <title>Ordinal scale variable</title>
        <statement>
          <p>
            An <alert>ordinal scale variable</alert> or <alert>rank</alert> is the results of a
            measurement, where there is a natural, meaningful way to order or rank the different
            outcome possibilities. E.g. finishing position in a race, education status.
          </p>
        </statement>
      </definition>

      <p>
        The <em>quantitative</em> difference between the outcomes might be unknown or uneven.
        E.g. education status: finishing elementary school takes 8 years in Europe, while an
        undergraduate degree is usually 3 years long; or you <em>can</em> say that the person who
        finished first was faster than the person who finished second, but you don't know the
        exact difference (unless you had a <q>finished at</q> timestamp data, which in turn is no
        longer an ordinal variable anymore). The important thing is that there is a natural,
        meaningful way to <em>order the outcomes</em>, but we don't <em>quantify</em> the
        difference between them.
      </p>

      <p>
        Note that you can have a numeric code assigned to an ordinal variable. However, do not
        process these numbers as if they were meaningful beyond them representing an order. E.g.,
        if you have a variable that measures the level of education, and you code it as
        1: elementary school, 2: high school, 3: undergraduate degree, 4: graduate degree, but
        the variable still represents an order (i.e., level): you cannot add, subtract, divide or
        multiply these numbers.
      </p>

      <example xml:id="ex-ordinal">
        <title>Ordinal scale variable example</title>
        <p>
          Suppose we're interested in people's attitudes to climate change, and we ask them to
          pick one of these four statements that most closely matches their beliefs:
        </p>
        <ol>
          <li><p>Temperatures are rising because of human activity</p></li>
          <li><p>Temperatures are rising, but we don't know why</p></li>
          <li><p>Temperatures are rising, but not because of humans</p></li>
          <li><p>Temperatures are not rising</p></li>
        </ol>
        <p>
          Notice that these four statements actually do have a natural ordering in terms of
          <q>the extent to which they agree with the current science</q>. Statement 1 is a close
          match, statement 2 is a suitable match, statement 3 isn't a perfect match, and statement
          4 strongly opposes science. So, in terms of the thing we're interested in (the extent to
          which people endorse the science), we can order the items as 1 &gt; 2 &gt; 3 &gt; 4.
          Since this ordering exists, it would be peculiar to list the options like this:
        </p>
        <ol>
          <li value="3"><p>Temperatures are rising, but not because of humans</p></li>
          <li value="1"><p>Temperatures are rising because of human activity</p></li>
          <li value="4"><p>Temperatures are not rising</p></li>
          <li value="2"><p>Temperatures are rising, but we don't know why</p></li>
        </ol>
        <p>
          -- because it seems to violate the natural <q>structure</q> of the question.
        </p>
        <p>
          So, let's suppose I asked 100 people these questions and got the following answers:
        </p>
        <table xml:id="table-climate1">
          <title/>
          <tabular halign="center">
            <row header="yes">
              <cell halign="left">Response</cell>
              <cell>Number</cell>
            </row>
            <row>
              <cell halign="left">1 Temperatures are rising because of human activity</cell>
              <cell>51</cell>
            </row>
            <row>
              <cell halign="left">2 Temperatures are rising, but we don't know why</cell>
              <cell>20</cell>
            </row>
            <row>
              <cell halign="left">3 Temperatures are rising, but not because of humans</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell halign="left">4 Temperatures are not rising</cell>
              <cell>19</cell>
            </row>
          </tabular>
        </table>
        <p>
          When analysing these data, it seems quite reasonable to try to group (1), (2) and (3)
          together and say that 81 of 100 people were willing to <em>at least partially</em>
          endorse the science. And it's <em>also</em> quite reasonable to group (2), (3) and (4)
          together and say that 49 of 100 people registered <em>at least some disagreement</em>
          with the dominant scientific view. However, it would be entirely bizarre to try to group
          (1), (2) and (4) together and say that 90 of 100 people said what? There's nothing
          sensible that allows you to group those responses together at all.
        </p>
        <p>
          That said, notice that while we <em>can</em> use the natural ordering of these items to
          construct sensible groupings, what we <em>can't</em> do is average them. For instance,
          in our simple example here, the <q>average</q> response to the question is 1.97. We
          would love to know if someone can tell us what that means.
        </p>
      </example>
    </subsection>

    <subsection xml:id="subsec-interval">
      <title>Interval Scale</title>

      <definition xml:id="def-interval">
        <title>Interval scale variable</title>
        <statement>
          <p>
            An (equal-)<alert>interval scale variable</alert> is the results of a
            <em>quantitative</em> measurement, where the difference between the outcomes is
            meaningful, but <em>no true zero</em> value can be assigned to our variable. E.g.
            temperature in Celsius or Fahrenheit etc.
          </p>
        </statement>
      </definition>

      <p>
        In contrast to nominal and ordinal scale variables, the <em>differences</em> between the
        numbers are interpretable: <em>addition</em> and <em>subtraction</em> make sense for
        interval scale variables. The intervals are same-sized, but a measurement value of 0 does
        not mean <q>nothing</q>/<q>none at all</q> on the Celsius scale: <m>0^\circ</m> means
        <q>the temperature at which water freezes</q>, it's a useful, but arbitrary label, not a
        true zero. As a consequence, it becomes pointless to try to multiply and divide
        temperatures. It is wrong to claim that <m>20^\circ</m> is <em>negative two times as
        hot as</em> <m>-10^\circ</m>.
      </p>

      <example xml:id="ex-interval">
        <title>Interval scale variable example</title>
        <p>
          Suppose we're interested in looking at how the attitudes of first-year university
          students have changed over time, and we need to capture the <em>year they started</em>.
          This is an interval scale variable. A student who started in 2003 did arrive 5 years
          before a student who started in 2008. However, it would be completely insane to divide
          2008 by 2003 and say that the second student started <q>1.0024 times later</q> than the
          first one.
        </p>
      </example>
    </subsection>

    <subsection xml:id="subsec-ratio">
      <title>Ratio Scale</title>

      <definition xml:id="def-ratio">
        <title>Ratio scale variable</title>
        <statement>
          <p>
            A <alert>ratio scale variable</alert> is the results of a <em>quantitative</em>
            measurement, where both the difference between the outcomes and the ratio of the
            outcomes are meaningful, and the variable has a true zero value. E.g. distance in
            meters, heart rate, mass, temperature on the Kelvin scale etc.
          </p>
        </statement>
      </definition>

      <p>
        The fourth and final type of variable to consider is a ratio scale variable, in which zero
        really means zero, and it's okay to multiply and divide on top of addition and subtraction.
        You can have a heart rate of zero, or in other words, <q>no heart rate at all</q> (an
        absolute zero), but that sadly means that you are likely dead. Kelvin is a ratio scale
        variable, because it has a true zero (absolute zero), and 100 K means truly twice as much
        energy as 50 K.
      </p>

      <example xml:id="ex-ratio">
        <title>Ratio scale variable example</title>
        <p>
          A psychological example would be the result of a short-term working memory capacity
          test<fn>One of the most famous ones is a digit span test
          (Miller <xref ref="millerMagicalNumberSeven1956"/>), but the original article is not
          without its faults (Cowan <xref ref="cowanGeorgeMillerMagical2015"/>).</fn>, where we
          ask respondents to remember a set of 5-letter words and recall them. Let's make our
          variable the number of words that they successfully recall. Person A is able to recall
          12 words, and Person B can recall 6 words, Person C cannot recall a single word
          (i.e., 0 words), and Person D can recall 7 words. We can set an <em>order</em> between
          them: Person A &gt; Person D &gt; Person B &gt; Person C; there is equal distance
          between the possible units, so subtraction is meaningful (<em>interval</em>); furthermore,
          there is a true zero (Person C). It also makes sense to say Person B made twice as many
          errors as Person A.
        </p>
      </example>
    </subsection>

    <subsection xml:id="subsec-likert">
      <title>The Special Case of the Likert Scale</title>

      <p>
        The humble Likert scale is all survey designs' bread and butter tool. You have filled out
        hundreds, maybe thousands of them, and odds are you've even used one yourself. Suppose we
        have a survey question that looks like this:
      </p>

      <blockquote>
        <p>
          Which of the following best describes your opinion of the statement that <q>all pirates
          are awesome</q> ...
        </p>
      </blockquote>

      <p>and then, the options presented to the participant are these:</p>

      <ol>
        <li><p>Strongly disagree</p></li>
        <li><p>Disagree</p></li>
        <li><p>Neither agree nor disagree</p></li>
        <li><p>Agree</p></li>
        <li><p>Strongly agree</p></li>
      </ol>

      <p>
        This set of items is an example of a 5-point Likert scale: people are asked to choose
        among one of several (in this case, 5) clearly ordered possibilities, generally with a
        verbal descriptor given in each case. However, it's not necessary that all items be
        explicitly described. This is a perfect example of a 5-point Likert scale too:
      </p>

      <ol>
        <li><p>Strongly disagree</p></li>
        <li><p/></li>
        <li><p/></li>
        <li><p/></li>
        <li><p>Strongly agree</p></li>
      </ol>

      <p>
        Likert scales are convenient, if somewhat limited, tools. The question is, what kind of
        variable are they? They're obviously discrete since you can't give a response of 2.5.
        They're obviously not nominal scale since the items are ordered, and they're not ratio
        scale either since there's no natural zero<fn>And you can't cheat. You can code your
        Likert scale from 0 to 4 instead of 1 to 5, but these are arbitrary, just like deciding
        that the freezing temperature is labelled as 0.</fn>.
      </p>

      <p>
        But are they ordinal scale or interval scale? One argument says that we can't prove that
        the difference between <q>strongly agree</q> and <q>agree</q> is of the same size as the
        difference between <q>agree</q> and <q>neither agree nor disagree</q>. In fact, in
        everyday life, it's pretty apparent they're not the same. So this suggests that we ought
        to treat Likert scales as ordinal variables. On the other hand, we can argue that some
        participants will take the whole <q>on a scale from 1 to 5</q> part seriously, and they
        tend to act as if the differences between the five response options were equidistant. While
        theoretically it is not an interval scale, researchers treat it as a
        <alert>quasi-interval scale</alert>.
      </p>
    </subsection>

    <subsection xml:id="subsec-continuous-discrete">
      <title>Continuous versus Discrete Variables</title>

      <p>
        There's a second kind of distinction that you need to be aware of regarding what types of
        variables you can run into. This is the distinction between <em>continuous</em> and
        <em>discrete</em> data types.
      </p>

      <definition xml:id="def-continuous">
        <title>Continuous variables</title>
        <statement>
          <p>
            A <alert>continuous variable</alert> can take on any value on a spectrum, and it's
            logically possible to have a value in between.
          </p>
        </statement>
      </definition>

      <example xml:id="ex-continuous">
        <title>Continuous variable example</title>
        <p>
          Response time is continuous. If Alan takes 3.1 seconds and Ben takes 2.3 seconds to
          respond to a question, then Cameron's response time can lie in between by taking
          3.0 seconds. And, of course, it would also be possible for David to take 3.031 seconds
          to respond, meaning that his RT would lie in between Cameron's and Alan's. And while in
          practice, it might be impossible to measure RT that precisely, it's certainly possible
          in principle. Because we can always find a new value for RT in between any two other
          ones, we say that RT is continuous.
        </p>
      </example>

      <definition xml:id="def-discrete">
        <title>Discrete variables</title>
        <statement>
          <p>
            A <alert>discrete variable</alert> can take on a limited number of distinct values;
            there is no possible value in between.
          </p>
        </statement>
      </definition>

      <example xml:id="ex-discrete">
        <title>Discrete variable example</title>
        <p>
          Nominal scale variables are always discrete: there isn't a type of transportation that
          falls <q>in-between</q> trains and bicycles, not in the strict mathematical way that 2.3
          falls in between 2 and 3. So transportation type is discrete.
        </p>
        <p>
          Similarly, ordinal scale variables are always discrete: although <q>2nd place</q> does
          fall between <q>1st place</q> and <q>3rd place</q>, there's nothing that can logically
          fall in between <q>1st place</q> and <q>2nd place</q>.
        </p>
        <p>
          Interval scale and ratio scale variables can go either way. Temperature in degrees
          Celsius (an interval scale variable) is also continuous; however, the year you went to
          school (an interval scale variable) is discrete. There's no year between 2002 and 2003.
          The number of questions you get right on a true-or-false test (a ratio scale variable)
          is also discrete: since a true-or-false question doesn't allow you to be <q>partially
          correct</q>, there's nothing in between 5/10 and 6/10.
        </p>
      </example>

      <p>
        Note that some people might say <q>discrete variable</q> when they mean <q>nominal scale
        variable</q>. While all nominal scale variables are discrete, not all discrete variables
        are nominal.
      </p>
    </subsection>

    <subsection xml:id="subsec-summary-levels">
      <title>A Summary Guide for Levels of Measurement</title>

      <table xml:id="table-measurementlevels">
        <title/>
        <tabular>
          <row header="yes" bottom="medium">
            <cell halign="left"><alert>Variable types</alert></cell>
            <cell><alert>Nominal</alert></cell>
            <cell><alert>Ordinal</alert></cell>
            <cell><alert>Interval</alert></cell>
            <cell><alert>Ratio</alert></cell>
          </row>
          <row>
            <cell colspan="5"><em>Data types</em></cell>
          </row>
          <row>
            <cell halign="left">Discrete</cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row>
            <cell halign="left"><em>Examples</em></cell>
            <cell>gender, birthplace</cell>
            <cell>education, finishing position</cell>
            <cell>year of enrolment to university</cell>
            <cell>number of questions answered correctly</cell>
          </row>
          <row>
            <cell halign="left">Continuous</cell>
            <cell/>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row bottom="medium">
            <cell halign="left"><em>Examples</em></cell>
            <cell/>
            <cell/>
            <cell>attitude</cell>
            <cell>height, weight, heart rate, distance</cell>
          </row>
          <row>
            <cell colspan="5"><em>Properties</em></cell>
          </row>
          <row>
            <cell halign="left">Can be ordered or ranked</cell>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row>
            <cell halign="left">Equidistant units</cell>
            <cell/>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row bottom="medium">
            <cell halign="left">Has a meaningful, true zero</cell>
            <cell/>
            <cell/>
            <cell/>
            <cell><m>\checkmark</m></cell>
          </row>
          <row>
            <cell colspan="5"><em>Valid operations</em></cell>
          </row>
          <row>
            <cell halign="left"><m>+</m> addition, <m>-</m> subtraction</cell>
            <cell/>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row bottom="medium">
            <cell halign="left"><m>\times</m> multiplication, <m>\div</m> division</cell>
            <cell/>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row>
            <cell colspan="5"><em>Central tendency measures</em></cell>
          </row>
          <row>
            <cell halign="left">Mode</cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row>
            <cell halign="left">Median</cell>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
          <row>
            <cell halign="left">Mean</cell>
            <cell/>
            <cell/>
            <cell><m>\checkmark</m></cell>
            <cell><m>\checkmark</m></cell>
          </row>
        </tabular>
      </table>
    </subsection>
  </section>

  <section xml:id="sec-ivdv">
    <title>Independent and Dependent Variables</title>

    <p>
      Usually, when we do some research, we end up with lots of different variables. Then, when
      we analyse our data, we often try to explain some of the variables in terms of the other
      variables. It's essential to keep the two roles, <q>thing doing the explaining</q> and
      <q>thing being explained</q>, distinct. So let's be clear about this now. Firstly, we might
      as well get used to the idea of using mathematical symbols to describe variables since it's
      going to happen repeatedly. Let's denote the <q>to be explained</q> variable <m>Y</m>, and
      the variables <q>doing the explaining</q> as <m>X_1</m>, <m>X_2</m>, etc.
    </p>

    <p>
      Now, when we are doing analysis, we have different names for <m>X</m> and <m>Y</m>, since
      they play different roles. The classical names for these roles are
      <alert>independent variable</alert> (IV) and <alert>dependent variable</alert> (DV). The IV
      is the variable you use to explain (i.e., <m>X</m>) and the DV is the variable being
      explained (i.e., <m>Y</m>). The logic behind these names goes like this: if there is a
      relationship between <m>X</m> and <m>Y</m>, then we can say that <m>Y</m> depends on
      <m>X</m>, and if we have designed our study <q>properly</q>, then <m>X</m> isn't dependent
      on anything else. However, those names are horrible: they're hard to remember, and they're
      highly misleading because (a) the IV is never actually <q>independent of everything else</q>
      and (b) if there's no relationship, then the DV doesn't actually depend on the IV. And,
      because we're not the only people who think that IV and DV are just awful names, there are
      several alternatives that some find more appealing. The terms used in these notes are
      <em><alert>predictors</alert></em> and <em><alert>outcomes</alert></em>. The idea here is
      that you're trying to use <m>X</m> (the predictors) to make guesses about <m>Y</m> (the
      outcomes). This is summarised in <xref ref="table-ivdv"/>.
    </p>

    <table xml:id="table-ivdv">
      <title>
        The terminology used to distinguish between different roles that a variable can play when
        analysing a data set
      </title>
      <tabular halign="left">
        <row header="yes">
          <cell>role of the variable</cell>
          <cell>classical name</cell>
          <cell>modern name</cell>
        </row>
        <row>
          <cell>to be explained</cell>
          <cell>dependent variable (DV)</cell>
          <cell>outcome</cell>
        </row>
        <row>
          <cell>to do the explaining</cell>
          <cell>independent variable (IV)</cell>
          <cell>predictor</cell>
        </row>
      </tabular>
    </table>
  </section>

  <section xml:id="sec-reliability">
    <title>Reliability</title>

    <p>
      By applying psychological measures we end up with variables, which can come in many
      different types. But the inevitable question arises: is the measurement any good? We'll do
      this in terms of two related ideas: <em>reliability</em> and <em>validity</em>.
    </p>

    <definition xml:id="def-reliability">
      <title>Reliability</title>
      <statement>
        <p>
          The <alert>reliability</alert> of a measure is the extent to which it is dependably,
          consistently, and stably giving the same result when measuring the same observation.
        </p>
      </statement>
    </definition>

    <example xml:id="ex-reliability-a">
      <title>Reliability example A</title>
      <p>
        You are designing a health psychology experiment to test the effect of psilocybin on mood
        disorders<fn>Read up on actual research on dosing, if interested. At the time of writing,
        this is an excellent starting point: Garcia-Romeu et al.
        <xref ref="garcia-romeuOptimalDosingPsilocybin2021"/>.</fn>. You are trying to find the
        right bodyweight-adjusted dose. You have a scale in the lab in Room A. You ask a
        participant to step on and you read 65 kg. For some reason, within a minute (in which they
        didn't drink, eat or go to the bathroom), you have to ask them to step back, but this time,
        the scale shows 74 kg. Disbelief creeps in, rightly. You are confused, and you decide to
        ask them to step on the scale again. This time, the scale shows 65 kg. Can you trust this
        65 kg reading?
      </p>
      <p>
        You borrow a scale from Room B and ask the same participant to step on it -- again, within
        a minute or two, so no weight shift should occur. This time, the scale shows 68 kg after
        three consecutive readings. But can you trust this 68 kg reading? You have your doubts.
        So you go into Room C, which has a different type of scale. The participant steps on the
        scale three times, and it consistently shows 68 kg.
      </p>
    </example>

    <example xml:id="ex-reliability-b">
      <title>Reliability example B</title>
      <p>
        You have a patient admitted to a ward, and you and your colleague are asked to evaluate
        them. You both use the same version of the Questionnaire X, still within the same time of
        day, but you both end up with different scores and different diagnoses. Can you trust either
        of your scores?
      </p>
      <p>
        A senior colleague provides you with SCID-5, a structured clinical interview for diagnosing
        mental disorders based on DSM-5. You use it to evaluate the patient. You both get a
        diagnosis of major depressive disorder.
      </p>
    </example>

    <p>
      The examples already hints at different ways of thinking about reliability, but let's
      summarise what the different types of reliability might be:
    </p>

    <ul>
      <li>
        <p>
          <em><alert>Test-retest</alert></em> or <em><alert>temporal reliability</alert></em>.
          This relates to consistency over time: if we repeat the measurement on the same thing at
          a later date, do we get the same answer? In <xref ref="ex-reliability-a"/>, the scale in
          Room A was not reliable, but the scale in Room B and Room C was.
        </p>
      </li>
      <li>
        <p>
          <em><alert>Interrater reliability</alert></em>. This relates to consistency across
          people: if someone else repeats the measurement, will they produce the same answer? In
          <xref ref="ex-reliability-b"/>, the two raters gave different answers with the same
          Questionnaire X, so clearly it was not reliable in this sense, however the SCID-5
          was<fn>SCID-5 is a golden standard, but it is not fully void of inter-rater reliability
          issues, particularly among recently trained clinicians (Brodey et al.
          <xref ref="brodeyValidationNetSCIDAutomated2016"/>), however, there are always new tools
          or versions to help tackle this.</fn>. If this fails, the instrument is subjective.
        </p>
      </li>
      <li>
        <p>
          <em><alert>Internal consistency reliability</alert></em> or
          <em><alert>homogeneity</alert></em>. Suppose a measurement is constructed from many
          different parts that perform similar functions. Inventories and scales use multiple
          questions to measure a single concept. If the questions are all measuring the same thing,
          then they should be consistent with each other. Ideally, items relating to the same
          concept should be highly correlated with each other, i.e., they should be consistent
          with each other<fn>Of course if some items are too highly correlated, one or two of them
          might even be redundant to be dropped while keeping the overall consistency high. However,
          we won't cover in detail survey design in this book.</fn>.
        </p>
      </li>
      <li>
        <p>
          <em><alert>Parallel/alternative forms reliability</alert></em>. This relates to
          consistency across theoretically-equivalent measurements. In
          <xref ref="ex-reliability-a"/>, the scales in both Room B and Room C gave the same
          reading. Two different <q>versions</q> of a measurement instrument were used to measure
          the same concept (human body weight), and they gave the same answer<fn>Making different
          versions of the same test, e.g. by switching up the order of items is one possible way
          to test for reliability.</fn>.
        </p>
      </li>
      <li>
        <p>
          <em><alert>Split-half reliability</alert></em>. This is a very specific concept widely
          used in survey design: divide a test into two halves, and see if the two halves are
          consistent with each other by calculating the reliability coefficient.
        </p>
      </li>
    </ul>

    <p>
      Not all measurements need to possess all forms of reliability. Nevertheless, it is important
      to be aware of reliability as a concept with its forms. CogStat will help you with analysing
      internal consistency and interrater reliability as of version 2.4+, but we will cover the
      tutorial once we understand how to compare two groups of data
      (<xref ref="ch-correlation"/>).
    </p>
  </section>

  <section xml:id="sec-validity">
    <title>Validity</title>

    <p>
      More than any other thing, a scientist wants their research to be <q>valid</q>. The
      conceptual idea behind validity is simple: can you trust the results of your study? In
      practice, there are different kinds of validity, each of which raises its own issues, and
      not all forms of validity are relevant to all studies.
    </p>

    <p>
      While validity is a research methodology subject and not a statistical one, some forms of
      validity are measured using statistical methods. So let's talk about different kinds of
      validity briefly, in a somewhat particular order.
    </p>

    <definition xml:id="def-content-validity">
      <title>Content validity</title>
      <statement>
        <p>
          The <alert>content validity</alert> is the extent to which an instrument measures the
          desired concept or construct comprehensively (with no gaps or missing aspects) and
          accurately (with no irrelevant or misleading domains).
        </p>
      </statement>
    </definition>

    <p>Some typical questions would be:</p>

    <ul>
      <li><p>Are all important domains of the concept we want to measure covered?</p></li>
      <li>
        <p>
          Does the selected instrument measure the same concept that we are trying to measure in
          all its aspects?
        </p>
      </li>
      <li>
        <p>
          Are there any items in the instrument that are not relevant to the concept we are trying
          to measure?
        </p>
      </li>
      <li><p>What biases might be present in the instrument?</p></li>
      <li>
        <p>
          Is the instrument culturally sensitive? Is the instrument reliable and consistent across
          different subgroups of the population?
        </p>
      </li>
    </ul>

    <p>
      A subjective judgement of content validity is called face validity.
    </p>

    <definition xml:id="def-face-validity">
      <title>Face validity</title>
      <statement>
        <p>
          The <alert>face validity</alert> of a study is the extent to which the measurement of a
          variable <q>looks like</q> it's measuring the correct theoretical construct, but it is a
          subjective judgement.
        </p>
      </statement>
    </definition>

    <p>Types of questions that you might ask:</p>

    <ul>
      <li><p>Does the instrument look like it's measuring the right thing?</p></li>
      <li>
        <p>
          Will the participant recognise the construct that we are trying to measure? (This might
          lead to cases where the participant fakes the response.)
        </p>
      </li>
    </ul>

    <definition xml:id="def-construct-validity">
      <title>Construct validity</title>
      <statement>
        <p>
          The <alert>construct validity</alert> is the extent to which the measurement of a
          variable is consistent with the theoretical construct that it is supposed to measure.
        </p>
      </statement>
    </definition>

    <p>Typical questions you might consider:</p>

    <ul>
      <li><p>Is the construct defined in a way that is clear and unambiguous?</p></li>
      <li><p>Is the construct defined too broadly or too narrowly?</p></li>
      <li><p>Is the sample and method appropriate for the construct?</p></li>
      <li>
        <p>
          How does this instrument relate to other instruments designed to measure the same
          construct?
        </p>
      </li>
      <li>
        <p>
          Does the instrument produce the same expected results as other instruments designed to
          measure the same construct?
        </p>
      </li>
      <li>
        <p>
          Does the instrument predict other variables that are theoretically related to the
          construct?
        </p>
      </li>
    </ul>

    <p>
      To test for construct validity, we can test for discriminant validity<fn>Sometimes referred
      to as <em>divergent validity</em>. This, however, is not a universally accepted terminology
      (Hubley <xref ref="hubleyDivergentValidity2014"/>).</fn> through correlation and factor
      analysis.
    </p>

    <definition xml:id="def-discriminant-validity">
      <title>Discriminant validity</title>
      <statement>
        <p>
          The <alert>discriminant validity</alert> assesses the extent to which a measure is
          capable of accurately distinguishing the construct variable it is intended to measure
          from an unrelated construct variables. It is the opposite of convergent validity.
        </p>
      </statement>
    </definition>

    <p>Question:</p>

    <ul>
      <li>
        <p>
          What is the correlation between the variable we are measuring and other variables that
          should not be related to it? <m>\rightarrow</m> <em>discriminant validity coefficient</em>
        </p>
      </li>
    </ul>

    <definition xml:id="def-convergent-validity">
      <title>Convergent validity</title>
      <statement>
        <p>
          The <alert>convergent validity</alert> is the extent to which the measurement of a
          variable is <em>consistent with other variables</em> that should be theoretically related
          to it.
        </p>
      </statement>
    </definition>

    <p>Types of questions you might consider:</p>

    <ul>
      <li>
        <p>
          What is the correlation between the variable we are measuring and other variables that
          should be related to it? <m>\rightarrow</m> <em>correlation coefficient</em>
        </p>
      </li>
    </ul>

    <example xml:id="ex-construct-validity">
      <title>Constructs and validity</title>
      <p>
        You are researching the relationship between socio-economic status and health outcomes.
        The health outcome is the construct of interest, it is the dependent variable. The
        socio-economic status is the independent variable. You are interested in the relationship
        between the two, and you want to know if socio-economic status is a good predictor of
        health outcomes.
      </p>
      <p>
        There are a number of related variables that you could use to measure socio-economic
        status, such as: personal income, household income, education, occupation, and so on. You
        could use any of these variables to measure socio-economic status, but you want to know
        which one is the best predictor of health outcomes. You could test for the relationship
        between each of these variables and health outcomes, and then choose the one that has the
        highest correlation coefficient. This is a form of convergent validity.
      </p>
      <p>
        There are unrelated variables that have an effect on health outcomes, such as: environment,
        gender, disability and impairments, minority status, family history of diseases, and so on.
        If the unrelated variables demonstrate a weak correlation with the socio-economic status
        variables, then this is a form of discriminant validity.
      </p>
      <p>
        You'll notice that in this particular example, gender<fn>E.g. research suggests that there
        are a number of reasons why women tend to live longer than men beyond socio-economic status
        (with which the relationship is negative), like biological protective factors and
        differences in risk aversion between men and women (Taylor and Stanton
        <xref ref="taylorHealthPsychology2021"/>).</fn>, disability, and minority status as
        variables would likely have a stronger than weak correlation with socio-economic status.
        These are called <alert>confounding variables</alert>. They are variables that are related
        to both the independent and dependent variables, and they can affect the results of the
        study. You'll need to control for these variables in your study design.
      </p>
    </example>

    <definition xml:id="def-internal-validity">
      <title>Internal validity</title>
      <statement>
        <p>
          The <alert>internal validity</alert> is the extent to which the results of the study can
          be attributed to the cause-and-effect relationships between the variables studied, rather
          than other factors. I.e., any change in the dependent variable (outcome) is a result of
          the manipulation of the independent variable (predictor), and not due to other factors.
        </p>
      </statement>
    </definition>

    <p>Typical questions would be:</p>

    <ul>
      <li><p>Is the relationship between the independent and dependent variables causal?</p></li>
      <li>
        <p>
          Is this cause-and-effect relationship the only one that could explain the results?
        </p>
      </li>
      <li>
        <p>Are there any confounding variables that could affect the results?</p>
      </li>
      <li><p>Was the design fit for establishing causality?</p></li>
    </ul>

    <definition xml:id="def-external-validity">
      <title>External validity</title>
      <statement>
        <p>
          The <alert>external validity</alert> of a study is the extent to which the results of
          the study can be generalised to other people, other situations, and other times.
        </p>
      </statement>
    </definition>

    <p>Questions to consider:</p>

    <ul>
      <li>
        <p>
          To what extent can the results of the study be generalised to other people, other
          situations, and other times?
        </p>
      </li>
      <li><p>Is the sample representative of the population?</p></li>
      <li><p>Is the sample too narrow? (e.g. only psychology students)</p></li>
      <li><p>Is this study replicable across different settings?</p></li>
      <li>
        <p>
          Are there any respondent biases that could affect the results? (Hawthorne effect,
          demand characteristics, etc.)
        </p>
      </li>
    </ul>

    <definition xml:id="def-ecological-validity">
      <title>Ecological validity</title>
      <statement>
        <p>
          The <alert>ecological validity</alert> of a study is the extent to which the entire
          study set-up closely approximates the real-world scenario being investigated.
        </p>
      </statement>
    </definition>

    <p>Some typical questions would be:</p>

    <ul>
      <li><p>Is the lab-based study set-up similar to the real-world scenario?</p></li>
      <li>
        <p>
          Would the study scenario occur naturally in the real world or in other non-controlled
          environment?
        </p>
      </li>
      <li>
        <p>
          Are there any environmental or systemic factors that could affect the replicability of
          results in the real world?
        </p>
      </li>
    </ul>

    <definition xml:id="def-criterion-validity">
      <title>Criterion validity</title>
      <statement>
        <p>
          The <alert>criterion validity</alert> of an instrument is the extent to which the score
          results are consistent with other measures of the same construct.
        </p>
      </statement>
    </definition>

    <p>Typical questions would be:</p>

    <ul>
      <li>
        <p>
          Does the instrument produce the same expected results as other instruments designed to
          measure the same construct?
        </p>
      </li>
      <li>
        <p>
          Is the correlation between the instrument and other instruments designed to measure the
          same construct high enough?
        </p>
      </li>
    </ul>

    <definition xml:id="def-concurrent-validity">
      <title>Concurrent validity</title>
      <statement>
        <p>
          The <alert>concurrent validity</alert> of an instrument is the extent to which the score
          results are consistent with other criterion measures (where the criterion is measured at
          the same time as the instrument is administered).
        </p>
      </statement>
    </definition>

    <p>Question:</p>

    <ul>
      <li>
        <p>
          What is the correlation between the instrument and other instruments designed to measure
          the same construct at the same time on the same sample? <m>\rightarrow</m>
          <em>concurrent validity coefficient</em>
        </p>
      </li>
    </ul>

    <definition xml:id="def-predictive-validity">
      <title>Predictive validity</title>
      <statement>
        <p>
          The <alert>predictive validity</alert> of an instrument is the extent to which the
          results can be used to make inferences about future criterion outcomes (where the
          criterion is measured after the instrument is administered).
        </p>
      </statement>
    </definition>

    <definition xml:id="def-postdictive-validity">
      <title>Postdictive validity</title>
      <statement>
        <p>
          The <alert>postdictive validity</alert> or <alert>retrospective validity</alert> of an
          instrument is the extent to which the results can be used to make inferences about past
          criterion outcomes (where the criterion is measured before the instrument is
          administered).
        </p>
      </statement>
    </definition>

    <remark xml:id="remark-modern-validity">
      <title>Modern Validity Theory Terminology</title>
      <p>
        The American Educational Research Association (AERA), the American Psychological
        Association (APA), and the National Council on Measurement in Education (NCME) prepared
        their Standards for Educational and Psychological Testing
        <xref ref="americaneducationalresearchassociationStandardsEducationalPsychological2014"/>.
        This standard uses different terminology to describe the different types of validity.
      </p>
      <p>
        It does not apply validity to the research instrument but rather the interpretation of it;
        it is a continuum and not a dichotomy (<q>valid</q> or <q>not valid</q>) (Edwards et al.
        <xref ref="edwardsFitPurposeModern2018"/>). So in this mindset, we no longer have types of
        validity but rather <em>types of validity evidence</em>:
      </p>
      <ul>
        <li><p>Evidence based on test content</p></li>
        <li><p>Evidence based on response processes</p></li>
        <li><p>Evidence based on internal structure</p></li>
        <li><p>Evidence based on relations to other variables</p></li>
      </ul>
      <p>
        However, still many statistics and research methodology textbooks used today, particularly
        the ones that are used to create this chapter, use the historical terminology.
      </p>
    </remark>
  </section>

  <section xml:id="sec-ch2-summary">
    <title>Summary</title>

    <p>
      This chapter isn't really meant to provide a comprehensive discussion of psychological
      research methods: it would require another volume just as long as this one does justice to
      the topic. However, in real life, statistics and study design are tightly intertwined, so
      discussing some key topics is convenient. In this chapter, we've briefly discussed the
      following:
    </p>

    <ul>
      <li>
        <p>
          <em>Measurement</em> (<xref ref="sec-measurement"/>). What does it mean to operationalise
          a theoretical construct? What does it mean to have variables and take measurements?
        </p>
      </li>
      <li>
        <p>
          <em>Scales of measurement and variable types</em> (<xref ref="sec-scales"/>). Remember
          that there are <em>two</em> different distinctions here: there's the difference between
          discrete and continuous data, and there's the difference between the four different scale
          types (nominal, ordinal, interval and ratio).
        </p>
      </li>
      <li>
        <p>
          <em>Terminology: predictors and outcomes</em> (<xref ref="sec-ivdv"/>). What roles do
          variables play in an analysis? Can you remember the difference between predictors and
          outcomes? Dependent and independent variables? Etc.
        </p>
      </li>
      <li>
        <p>
          <em>Reliability</em> (<xref ref="sec-reliability"/>). Can you trust your results? How do
          you know that your measurements are consistent?
        </p>
      </li>
      <li>
        <p>
          <em>Validity</em> (<xref ref="sec-validity"/>). Does your study measure what you want
          it to?
        </p>
      </li>
    </ul>
  </section>

</chapter>
