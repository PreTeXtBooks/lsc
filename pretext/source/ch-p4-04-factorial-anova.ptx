<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch-factorial-anova" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Comparing Several Groups (Factorial ANOVA)</title>

  <introduction>
    <p>
      In past chapters, we compared two groups for the same variable, or we compared two variables.
      In this chapter, we are going to look at more than one grouping variable, which we sometimes refer
      to as <alert>factorial ANOVA</alert>.
    </p>
  </introduction>

  <!-- ============================================================ -->
  <!-- Section 1: Balanced designs                                  -->
  <!-- ============================================================ -->
  <section xml:id="sec-factorialanovasimple">
    <title>Balanced Designs</title>

    <p>
      When we discussed the analysis of variance in an earlier chapter, we assumed a fairly simple
      experimental design: each person falls into one of several groups, and we want to know whether these
      groups have different means on some outcome variable. In this section, we will look at a broader
      class of experimental designs, known as <alert>factorial designs</alert>, where we have more than
      one grouping variable.
    </p>
    <p>
      Let's take the example that appears in the one-way ANOVA chapter, in which we were looking at the
      effect of different drugs on the <c>mood_gain</c> experienced by each person. In that chapter, we
      did find a significant effect of <c>drug</c>, but at the end of the chapter we also ran an analysis
      to see if there was an effect of <c>therapy</c>. We didn't find one, but there's something a bit
      worrying about trying to run two <em>separate</em> analyses trying to predict the same outcome.
      Maybe there actually <em>is</em> an effect of therapy on mood gain, but we couldn't find it because
      it was being <q>hidden</q> by the effect of drug? In other words, we're going to want to run a
      <em>single</em> analysis that includes <em>both</em> <c>drug</c> and <c>therapy</c> as predictors.
    </p>
    <p>
      For this analysis, each person is cross-classified by the drug they were given (a factor with 3
      levels) and what therapy they received (a factor with 2 levels). We refer to this as a
      <m>3 \times 2</m> factorial design. Let's load the <c>clinicaltrial.csv</c> data set again to
      CogStat, and run the <c>Compare groups</c> function with both <c>drug</c> and <c>therapy</c> as
      grouping variables.
    </p>

    <figure xml:id="fig-cogstatcompareclinanova2">
      <caption>Running <c>Compare groups</c> with two grouping variables in CogStat</caption>
      <image source="cogstatcompareclinanova2.png" width="80%">
        <description>CogStat Compare groups dialog for clinical trial with drug and therapy as grouping variables</description>
      </image>
    </figure>

    <p>We get the following table:</p>

    <figure xml:id="fig-cogstatanova2clinload">
      <caption>Cross-tabulation of participants across drug and therapy conditions</caption>
      <image source="cogstatanova2clinload.png" width="80%">
        <description>CogStat cross-tabulation showing participants in each combination of drug and therapy</description>
      </image>
    </figure>

    <p>
      As you can see, not only do we have participants corresponding to all possible combinations of the
      two factors, indicating that our design is <alert>completely crossed</alert>, it turns out that
      there are an equal number of people in each group. In other words, we have a <alert>balanced</alert>
      design.
    </p>

    <subsection xml:id="subsec-factanovahyp">
      <title>What Hypotheses Are We Testing?</title>

      <p>
        Like one-way ANOVA, factorial ANOVA is a tool for testing certain types of hypotheses about
        population means. So a sensible place to start would be to be explicit about what our hypotheses
        actually are.
      </p>
      <p>
        However, before we can even get to that point, it's really useful to have some clean and simple
        ways to describe the means. Because of the fact that observations are cross-classified in terms
        of two different factors, we'll need a cross-tabulation of sorts.
      </p>
      <p>
        Now, this output by CogStat shows a cross-tabulation of the group means and other descriptive
        statistics for all possible combinations of the two factors (e.g. people who received the placebo
        and no therapy, people who received the placebo while getting CBT etc.), and it also shows a
        boxplot comparing these combinations.
      </p>

      <figure xml:id="fig-anova2clinicalbox">
        <caption>Boxplot comparing mood gain across all drug and therapy combinations</caption>
        <image source="cogstatanova2boxplotclinical.png" width="80%">
          <description>CogStat boxplot for clinical trial factorial ANOVA showing mood gain by drug and therapy combination</description>
        </image>
      </figure>

      <p>
        But sometimes, we want to dissect our data in a different output format. To do that, we have the
        <c>Pivot table</c> function in CogStat. Let's use it to create a cross-tabulation of the means
        of <c>mood_gain</c> for each combination of <c>drug</c> and <c>therapy</c>.
      </p>

      <figure xml:id="fig-anova2clinicalpivot">
        <caption>Cross-tabulation of the means of <c>mood_gain</c> for each combination of <c>drug</c>
        and <c>therapy</c> in <c>Pivot table</c> function. You can select different functions to
        tabulate: <em>N</em> (count), <em>Sum</em>, <em>Mean</em>, <em>Median</em>, <em>Lower</em> and
        <em>Upper quartile</em>, <em>Standard deviation</em>, and <em>Variance</em>.</caption>
        <sidebyside widths="48% 48%">
          <image source="cogstatpivotclinicaldialog.png">
            <description>CogStat Pivot table dialog for clinical trial data</description>
          </image>
          <image source="cogstatpivotclinicalpivot.png">
            <description>CogStat Pivot table output showing means for each drug-therapy combination</description>
          </image>
        </sidebyside>
      </figure>

      <p>
        Let's use some mathematical notation and the symbol <m>\mu</m> to denote a population mean.
        However, because there are lots of different means, we'll need to use subscripts to distinguish
        between them. Here's how the notation works. Our table is defined in terms of two factors: each
        row corresponds to a different level of Factor A (in this case, <c>drug</c>), and each column
        corresponds to a different level of Factor B (in this case, <c>therapy</c>). If we let <m>R</m>
        denote the number of rows in the table, and <m>C</m> denote the number of columns, we can refer
        to this as an <m>R \times C</m> factorial ANOVA. In this case <m>R=3</m> and <m>C=2</m>.
      </p>
      <p>
        We'll use lowercase letters to refer to specific rows and columns, so <m>\mu_{rc}</m> refers to
        the population mean associated with the <m>r</m>th level of Factor A (i.e. row number <m>r</m>)
        and the <m>c</m>th level of Factor B (column number <m>c</m>).<fn>The nice thing about the
        subscript notation is that generalises nicely: if our experiment had involved a third factor,
        then we could just add a third subscript. In principle, the notation extends to as many factors
        as you might care to include.</fn> We use the <q>dot</q> notation to express averages across
        rows and columns. In the case of Joyzepam, notice that we're talking about the mean associated
        with the third row in the table. That is, we're averaging across two cell means (i.e.,
        <m>\mu_{31}</m> and <m>\mu_{32}</m>). The result of this averaging is referred to as a
        <alert>marginal mean</alert>, and would be denoted <m>\mu_{3.}</m> in this case. The marginal
        mean for CBT corresponds to the population mean associated with the second column in the table,
        so we use the notation <m>\mu_{.2}</m> to describe it. The grand mean is denoted <m>\mu_{..}</m>
        because it is the mean obtained by averaging (marginalising<fn>Technically, marginalising isn't
        quite identical to a regular mean: it's a weighted average, where you take into account the
        frequency of the different events that you're averaging over. However, in a balanced design, all
        of our cell frequencies are equal by definition, so the two are equivalent.</fn>) over both. So
        our full table of population means can be written down like this:
      </p>

      <table xml:id="table-popn-means-factorial">
        <title>Population means in factorial ANOVA</title>
        <tabular halign="center">
          <row header="yes">
            <cell></cell>
            <cell>No therapy</cell>
            <cell>CBT</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell>Placebo</cell>
            <cell><m>\mu_{11}</m></cell>
            <cell><m>\mu_{12}</m></cell>
            <cell><m>\mu_{1.}</m></cell>
          </row>
          <row>
            <cell>Anxifree</cell>
            <cell><m>\mu_{21}</m></cell>
            <cell><m>\mu_{22}</m></cell>
            <cell><m>\mu_{2.}</m></cell>
          </row>
          <row>
            <cell>Joyzepam</cell>
            <cell><m>\mu_{31}</m></cell>
            <cell><m>\mu_{32}</m></cell>
            <cell><m>\mu_{3.}</m></cell>
          </row>
          <row>
            <cell>Total</cell>
            <cell><m>\mu_{.1}</m></cell>
            <cell><m>\mu_{.2}</m></cell>
            <cell><m>\mu_{..}</m></cell>
          </row>
        </tabular>
      </table>

      <p>
        Now that we have this notation, it is straightforward to formulate and express some hypotheses.
        Let's suppose that the goal is to find out two things: firstly, does the drug choice have any
        effect on mood, and secondly, does CBT have any effect on mood? These aren't the only hypotheses
        that we could formulate of course, but these are the two simplest hypotheses to test, and so
        we'll start there.
      </p>
      <p>
        Consider the first test. If the drug has no effect, then we would expect all of the row means
        to be identical, right? So that's our null hypothesis. On the other hand, if the drug does
        matter, then we should expect these row means to be different. Formally, we write down our null
        and alternative hypotheses in terms of the <em>equality of marginal means</em>:
      </p>

      <table xml:id="table-factorial-hyp-drug">
        <title>Null and alternative hypotheses for the main effect of <c>drug</c></title>
        <tabular>
          <row>
            <cell>Null hypothesis <m>H_0</m>:</cell>
            <cell>row means are the same i.e. <m>\mu_{1.} = \mu_{2.} = \mu_{3.}</m></cell>
          </row>
          <row>
            <cell>Alternative hypothesis <m>H_1</m>:</cell>
            <cell>at least one row mean is different.</cell>
          </row>
        </tabular>
      </table>

      <p>
        It's worth noting that these are <em>exactly</em> the same statistical hypotheses that we formed
        when we ran a one-way ANOVA on these data in the previous chapter. Back then, we used the
        notation <m>\mu_P</m> to refer to the mean mood gain for the placebo group, with <m>\mu_A</m>
        and <m>\mu_J</m> corresponding to the group means for the two drugs, and the null hypothesis was
        <m>\mu_P = \mu_A = \mu_J</m>. So we're actually talking about the same hypothesis: it's just
        that the more complicated ANOVA requires more careful notation due to the presence of multiple
        grouping variables, so we're now referring to this hypothesis as
        <m>\mu_{1.} = \mu_{2.} = \mu_{3.}</m>. However, as we'll see shortly, although the hypothesis
        is identical, the test of that hypothesis is subtly different due to the fact that we're now
        acknowledging the existence of the second grouping variable.
      </p>
      <p>
        Speaking of the other grouping variable, you won't be surprised to discover that our second
        hypothesis test is formulated the same way. However, since we're talking about the psychological
        therapy rather than drugs, our null hypothesis now corresponds to the equality of the column means:
      </p>

      <table xml:id="table-factorial-hyp-therapy">
        <title>Null and alternative hypotheses for the main effect of <c>therapy</c></title>
        <tabular>
          <row>
            <cell>Null hypothesis <m>H_0</m>:</cell>
            <cell>column means are the same, i.e., <m>\mu_{.1} = \mu_{.2}</m></cell>
          </row>
          <row>
            <cell>Alternative hypothesis <m>H_1</m>:</cell>
            <cell>column means are different, i.e., <m>\mu_{.1} \neq \mu_{.2}</m></cell>
          </row>
        </tabular>
      </table>

    </subsection>

    <subsection xml:id="subsec-factorial-ss">
      <title>Means, Sums of Squares, and Degrees of Freedom</title>

      <p>
        The null and alternative hypotheses might seem awfully familiar: they're basically the same as
        the hypotheses that we were testing in our simpler one-way ANOVAs. So you're probably expecting
        that the hypothesis <em>tests</em> that are used in factorial ANOVA will be essentially the same
        as the <m>F</m>-test from the one-way ANOVA chapter. You're expecting to see references to sums
        of squares (SS), mean squares (MS), degrees of freedom (df), and finally an <m>F</m>-statistic
        that we can convert into a <m>p</m>-value, right? Well, you're absolutely and completely right.
      </p>
      <p>
        We used <m>\mu</m> for population means, but we'll use <m>\bar{Y}</m> to refer to a sample mean.
        For the rest of the formulas, we can use the same notation as before to refer to group means,
        marginal means and grand means: that is, <m>\bar{Y}_{rc}</m> is the sample mean associated with
        the <m>r</m>th level of Factor A and the <m>c</m>th level of Factor B, <m>\bar{Y}_{r.}</m>
        would be the marginal mean for the <m>r</m>th level of Factor A, <m>\bar{Y}_{.c}</m> would be
        the marginal mean for the <m>c</m>th level of Factor B, and <m>\bar{Y}_{..}</m> is the grand
        mean. In other words, our sample means can be organised into the same table as the population
        means. For our clinical trial data, that table looks like this:
      </p>

      <table xml:id="table-sample-means-factorial">
        <title>Sample means in factorial ANOVA notation</title>
        <tabular halign="center">
          <row header="yes">
            <cell></cell>
            <cell>No therapy</cell>
            <cell>CBT</cell>
            <cell>Total</cell>
          </row>
          <row>
            <cell>Placebo</cell>
            <cell><m>\bar{Y}_{11}</m></cell>
            <cell><m>\bar{Y}_{12}</m></cell>
            <cell><m>\bar{Y}_{1.}</m></cell>
          </row>
          <row>
            <cell>Anxifree</cell>
            <cell><m>\bar{Y}_{21}</m></cell>
            <cell><m>\bar{Y}_{22}</m></cell>
            <cell><m>\bar{Y}_{2.}</m></cell>
          </row>
          <row>
            <cell>Joyzepam</cell>
            <cell><m>\bar{Y}_{31}</m></cell>
            <cell><m>\bar{Y}_{32}</m></cell>
            <cell><m>\bar{Y}_{3.}</m></cell>
          </row>
          <row>
            <cell>Total</cell>
            <cell><m>\bar{Y}_{.1}</m></cell>
            <cell><m>\bar{Y}_{.2}</m></cell>
            <cell><m>\bar{Y}_{..}</m></cell>
          </row>
        </tabular>
      </table>

      <p>
        If we look at the sample means from earlier (<xref ref="fig-anova2clinicalpivot"/>), we have
        <m>\bar{Y}_{11} = 0.30</m>, <m>\bar{Y}_{12} = 0.60</m> etc. In our clinical trial example, the
        <c>drugs</c> factor has 3 levels and the <c>therapy</c> factor has 2 levels, and so what we're
        trying to run is a <m>3 \times 2</m> factorial ANOVA.
      </p>
      <p>
        To be a little more general, we can say that Factor A (the row factor) has <m>R</m> levels and
        Factor B (the column factor) has <m>C</m> levels, and so what we're running here is an
        <m>R \times C</m> factorial ANOVA. The formula for the sum of squares values for each of the two
        factors is relatively familiar. For Factor A, our between group sum of squares is calculated by
        assessing the extent to which the (row) marginal means <m>\bar{Y}_{1.}</m>, <m>\bar{Y}_{2.}</m>
        etc, are different from the grand mean <m>\bar{Y}_{..}</m>:
        <me>\mbox{SS}_{A} = (N \times C)  \sum_{r=1}^R  \left( \bar{Y}_{r.} - \bar{Y}_{..} \right)^2</me>
        The formula for factor B is, of course, the same thing, just with some subscripts shuffled around:
        <me>\mbox{SS}_{B} = (N \times R) \sum_{c=1}^C \left( \bar{Y}_{.c} - \bar{Y}_{..} \right)^2</me>
      </p>
      <p>
        Okay, now let's calculate the sum of squares associated with the main effect of <c>drug</c>.
        There are a total of <m>N=3</m> people in each group, and <m>C=2</m> different types of therapy.
        Or, to put it another way, there are <m>3 \times 2 = 6</m> people who received any particular
        drug. So our calculations are:
        <md>
          <mrow>\mbox{SS}_{drug} \amp= (N \times C)  \sum_{r=1}^R  \left( \bar{Y}_{r.} - \bar{Y}_{..} \right)^2</mrow>
          <mrow>\amp= 3.453333</mrow>
        </md>
      </p>
      <p>
        We can repeat the same kind of calculation for the effect of therapy. Again there are <m>N=3</m>
        people in each group, but since there are <m>R=3</m> different drugs, this time around we note
        that there are <m>3 \times 3 = 9</m> people who received CBT, and an additional 9 people who
        received the placebo. So our calculation is now:
        <md>
          <mrow>\mbox{SS}_{therapy} \amp= (N \times C)  \sum_{r=1}^R  \left( \bar{Y}_{r.} - \bar{Y}_{..} \right)^2</mrow>
          <mrow>\amp= 0.467222</mrow>
        </md>
      </p>
      <p>
        So that's how you calculate the SS values for the two main effects. These SS values are analogous
        to the between-group sum of squares values that we calculated when doing one-way ANOVA. However,
        it's not a good idea to think of them as between-groups SS values anymore, just because we have
        two different grouping variables and it's easy to get confused. In order to construct an <m>F</m>
        test, however, we also need to calculate the within-groups sum of squares. We will refer to the
        within-groups SS value as the <em>residual</em><fn>This will be a term used and explained in a
        later chapter on regression.</fn> sum of squares <m>\mbox{SS}_R</m>.
      </p>
      <p>
        The easiest way to think about the residual SS values in this context, is to think of it as the
        leftover variation in the outcome variable after you take into account the differences in the
        marginal means (i.e. after you remove <m>\mbox{SS}_{drug}</m> and <m>\mbox{SS}_{therapy}</m>).
        What I mean by that is we can start by calculating the total sum of squares (<m>\mbox{SS}_T</m>).
        We take the difference between each observation <m>Y_{rci}</m> and the grand mean
        <m>\bar{Y}_{..}</m>, square the differences, and add them all up
        <md>
          <mrow>\mbox{SS}_T \amp= \sum_{r=1}^R \sum_{c=1}^C \sum_{i=1}^N \left( Y_{rci} - \bar{Y}_{..}\right)^2</mrow>
          <mrow>\amp= 4.845</mrow>
        </md>
      </p>
      <p>
        The <q>triple summation</q> here looks more complicated than it is. In the first two summations,
        we're summing across all levels of Factor A (i.e., over all possible rows <m>r</m> in our table),
        across all levels of Factor B (i.e. all possible columns <m>c</m>). Each <m>rc</m> combination
        corresponds to a single group, and each group contains <m>N</m> people: so we have to sum across
        all those people (i.e. all <m>i</m> values) too. In other words, all we're doing here is summing
        across all observations in the data set (i.e. all possible <m>rci</m> combinations).
      </p>
      <p>
        The residual sum of squares is thus defined to be the variability in <m>Y</m> that <em>can't</em>
        be attributed to either of our two factors. In other words:
        <md>
          <mrow>\mbox{SS}_R \amp= \mbox{SS}_T - (\mbox{SS}_A + \mbox{SS}_B)</mrow>
          <mrow>\amp= 4.845 - (3.45333 + 0.46722)</mrow>
          <mrow>\amp= 0.92445</mrow>
        </md>
      </p>
      <p>
        It is commonplace to refer to <m>\mbox{SS}_A + \mbox{SS}_B</m> as the variance attributable to
        the <q>ANOVA model</q>, denoted <m>\mbox{SS}_M</m>, and so we often say that the total sum of
        squares is equal to the model sum of squares plus the residual sum of squares.
      </p>
      <p>
        The degrees of freedom are calculated in much the same way as for one-way ANOVA. The degrees of
        freedom equals the number of quantities that are observed, minus the number of constraints. So,
        for the <c>drugs</c> factor, we observe 3 separate group means, but these are constrained by 1
        grand mean; and therefore the degrees of freedom is <m>df = 2</m>. For the <c>therapy</c> factor
        we obtain <m>df=1</m>.
      </p>
      <p>
        For the residuals, the logic is similar, but not quite the same. The total number of observations
        in our experiment is 18. The constraints correspond to the 1 grand mean, the 2 additional group
        means that the <c>drug</c> factor introduces, and the 1 additional group mean that the the
        <c>therapy</c> factor introduces, and so our degrees of freedom is 14. As a formula, this is
        <m>N-1 -(R-1)-(C-1)</m>, which simplifies to <m>N-R-C+1</m>.
      </p>
      <p>
        Just like we saw with the original one-way ANOVA, note that the mean square value is calculated
        by dividing SS by the corresponding <m>df</m>. That is, it's still true that
        <me>\mbox{MS} = \frac{\mbox{SS}}{df}</me>
        regardless of whether we're talking about <c>drug</c>, <c>therapy</c> or the residuals. To see
        this, let's not worry about how the sums of squares values are calculated. So for the <c>drug</c>
        factor, we divide <m>3.45333</m> by <m>2</m>, and end up with a mean square value of
        <m>1.73</m>. For the <c>therapy</c> factor, there's only 1 degree of freedom, so our
        calculations are even simpler: dividing <m>0.46722</m> (the SS value) by 1 gives us an answer of
        <m>0.47</m> (the MS value).
      </p>
      <p>
        Turning to the <m>F</m> statistics and the <m>p</m> values, notice that we have two of each: one
        corresponding to the <c>drug</c> factor and the other corresponding to the <c>therapy</c> factor.
        Regardless of which one we're talking about, the <m>F</m> statistic is calculated by dividing the
        mean square value associated with the factor by the mean square value associated with the
        residuals:
        <me>F_{A} = \frac{\mbox{MS}_{A}}{\mbox{MS}_{R}}</me>
        and an equivalent formula exists for factor B (i.e. <c>therapy</c>).
      </p>
      <p>
        So for the <c>drug</c> factor, we take the mean square of <m>1.73</m> and divide it by the
        residual mean square value of <m>0.07</m>, which gives us an <m>F</m>-statistic of <m>26.15</m>.
        The corresponding calculation for the <c>therapy</c> variable would be to divide <m>0.47</m> by
        <m>0.07</m> which gives <m>7.08</m> as the <m>F</m>-statistic.
      </p>
      <p>
        Regarding the <m>p</m>-value, what we're trying to do is test the null hypothesis that there is
        no relationship between the factor and the outcome variable. To that end, we've followed a
        similar strategy that we did in the one-way ANOVA, and have calculated an <m>F</m>-statistic for
        each of these hypotheses. To convert these to <m>p</m> values, all we need to do is note that
        the sampling distribution for the <m>F</m> <em>statistic</em> under the null hypothesis is an
        <m>F</m> <em>distribution</em>, and that two degrees of freedom values are those corresponding
        to the factor, and those corresponding to the residuals. For the <c>drug</c> factor we're
        talking about an <m>F</m> distribution with 2 and 14 degrees of freedom. In contrast, for the
        <c>therapy</c> factor sampling distribution is <m>F</m> with 1 and 14 degrees of freedom.
      </p>
      <p>
        So, for the <c>drug</c> factor, we have an <m>F</m>-statistic of <m>26.15</m> and an
        <m>F</m>-distribution with 2 and 14 degrees of freedom. The corresponding <m>p</m>-value is
        <m>\lt 0.001</m>. For the <c>therapy</c> factor, we have an <m>F</m>-statistic of <m>7.08</m>
        and an <m>F</m>-distribution with 1 and 14 degrees of freedom. The corresponding <m>p</m>-value
        is <m>0.02</m>.
      </p>
      <p>
        But hang on! You've run the analysis in CogStat and you see something vastly different. That's
        okay. Bear with us for a moment.
      </p>

    </subsection>

    <subsection xml:id="subsec-factorial-interaction">
      <title>The Interaction</title>

      <p>
        The ANOVA model that we've been talking about so far covers a range of different patterns that
        we might observe in our data. For instance, in a two-way ANOVA design, there are four
        possibilities: (a) only Factor A matters, (b) only Factor B matters, (c) both A and B matter,
        and (d) neither A nor B matters. An example of each of these four possibilities is plotted in
        <xref ref="fig-maineffects"/>.
      </p>

      <figure xml:id="fig-maineffects">
        <caption>Factor main effects: four possible patterns in a two-way ANOVA</caption>
        <sidebyside widths="23% 23% 23% 23%">
          <image source="maineffectA.png">
            <description>Only Factor A matters</description>
          </image>
          <image source="maineffectB.png">
            <description>Only Factor B matters</description>
          </image>
          <image source="maineffectAB.png">
            <description>Both Factors A and B matter</description>
          </image>
          <image source="maineffectO.png">
            <description>Neither Factor A nor B matters</description>
          </image>
        </sidebyside>
      </figure>

      <p>
        The four patterns of data shown in <xref ref="fig-maineffects"/> are all quite realistic: there
        are a great many data sets that produce exactly those patterns. However, they are not the whole
        story, and the ANOVA model that we have been talking about up to this point is not sufficient to
        fully account for a table of group means. Why not? Well, so far we have the ability to talk about
        the idea that drugs can influence mood, and therapy can influence mood, but no way of talking
        about the possibility of an <alert>interaction</alert> between the two. An interaction between A
        and B is said to occur whenever the effect of Factor A is <em>different</em>, depending on which
        level of Factor B we're talking about. Several examples of an interaction effect with the context
        of a 2 x 2 ANOVA are shown in <xref ref="fig-interaction"/>.
      </p>

      <figure xml:id="fig-interaction">
        <caption>Qualitatively different interactions for a <m>2 \times 2</m> ANOVA</caption>
        <sidebyside widths="23% 23% 23% 23%">
          <image source="interaction1.png">
            <description>Interaction type 1</description>
          </image>
          <image source="interaction2.png">
            <description>Interaction type 2</description>
          </image>
          <image source="interaction3.png">
            <description>Interaction type 3</description>
          </image>
          <image source="interaction4.png">
            <description>Interaction type 4</description>
          </image>
        </sidebyside>
      </figure>

      <p>
        To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed
        quite different physiological mechanisms, and one consequence of this is that while Joyzepam has
        more or less the same effect on mood regardless of whether one is in therapy, Anxifree is
        actually much more effective when administered in conjunction with CBT. The ANOVA that we
        developed manually, does not capture this idea. To get some idea of whether an interaction is
        actually happening here, it helps to plot the various group means.
      </p>

      <figure xml:id="fig-interactionplot">
        <caption>Interaction plot for our clinical trial data</caption>
        <image source="interactionplot.png" width="80%">
          <description>Interaction plot showing mood gain by drug across therapy conditions</description>
        </image>
      </figure>

      <p>
        Our main concern relates to the fact that the two lines aren't parallel. The effect of CBT
        (difference between solid line and dotted line) when the drug is Joyzepam (right side) appears
        to be near zero, even smaller than the effect of CBT when a placebo is used (left side). However,
        when Anxifree is administered, the effect of CBT is larger than the placebo (middle). Is this
        effect real, or is this just random variation due to chance? Our original ANOVA cannot answer
        this question, because we make no allowances for the idea that interactions even exist! In this
        section, we'll fix this problem.
      </p>
      <p>
        Although there are only two <em>factors</em> involved in our model (i.e. <c>drug</c> and
        <c>therapy</c>), there are actually three distinct <alert>terms</alert> (i.e. <c>drug</c>,
        <c>therapy</c> and <c>drug × therapy</c>). That is, in addition to the main effects of
        <c>drug</c> and <c>therapy</c>, we have a new component to the model, which is our interaction
        term <c>drug × therapy</c>.
      </p>
      <p>
        Intuitively, the idea behind an interaction effect is fairly simple: it means that the effect of
        Factor A is different, depending on which level of Factor B we're talking about. But what does
        that actually mean in terms of our data? <xref ref="fig-interactionplot"/> depicts several
        different patterns that, although quite different to each other, would all count as an interaction
        effect. So it's not entirely straightforward to translate this qualitative idea into something
        mathematical that a statistician can work with. As a consequence, the way that the idea of an
        interaction effect is formalised in terms of null and alternative hypotheses is slightly
        difficult.
      </p>
      <p>
        To start with, we need to be a little more explicit about our main effects. Consider the main
        effect of Factor A (<c>drug</c> in our running example). We originally formulated this in terms
        of the null hypothesis that the two marginal means <m>\mu_{r.}</m> are all equal to each other.
        Obviously, if all of these are equal to each other, then they must also be equal to the grand
        mean <m>\mu_{..}</m> as well, right? So what we can do is define the <em>effect</em> of Factor A
        at level <m>r</m> to be equal to the difference between the marginal mean <m>\mu_{r.}</m> and
        the grand mean <m>\mu_{..}</m>. Let's denote this effect by <m>\alpha_r</m>, and note that
        <me>\alpha_r  = \mu_{r.} - \mu_{..}</me>
        Now, by definition all of the <m>\alpha_r</m> values must sum to zero, for the same reason that
        the average of the marginal means <m>\mu_{r.}</m> must be the grand mean <m>\mu_{..}</m>. We can
        similarly define the effect of Factor B at level <m>i</m> to be the difference between the
        column marginal mean <m>\mu_{.c}</m> and the grand mean <m>\mu_{..}</m>
        <me>\beta_c = \mu_{.c} - \mu_{..}</me>
        and once again, these <m>\beta_c</m> values must sum to zero. The reason that statisticians
        sometimes like to talk about the main effects in terms of these <m>\alpha_r</m> and
        <m>\beta_c</m> values is that it allows them to be precise about what it means to say that there
        is no interaction effect. If there is no interaction at all, then these <m>\alpha_r</m> and
        <m>\beta_c</m> values will perfectly describe the group means <m>\mu_{rc}</m>. Specifically, it
        means that
        <me>\mu_{rc} = \mu_{..} + \alpha_r + \beta_c</me>
        That is, there's nothing <em>special</em> about the group means that you couldn't predict
        perfectly by knowing all the marginal means. And that's our null hypothesis, right there. The
        alternative hypothesis is that
        <me>\mu_{rc} \neq \mu_{..} + \alpha_r + \beta_c</me>
        for at least one group <m>rc</m> in our table. However, statisticians often like to write this
        slightly differently. They'll usually define the specific interaction associated with group
        <m>rc</m> to be some number, awkwardly referred to as <m>(\alpha\beta)_{rc}</m>, and then they
        will say that the alternative hypothesis is that
        <me>\mu_{rc} = \mu_{..} + \alpha_r + \beta_c + (\alpha\beta)_{rc}</me>
        where <m>(\alpha\beta)_{rc}</m> is non-zero for at least one group. This notation is kind of
        ugly to look at, but it is handy to calculate the sum of squares.
      </p>
      <p>
        How should we calculate the sum of squares for the interaction terms,
        <m>\mbox{SS}_{A:B}</m>? For Factor A, a good way to estimate the main effect at level <m>r</m>
        as the difference between the <em>sample</em> marginal mean <m>\bar{Y}_{rc}</m> and the sample
        grand mean <m>\bar{Y}_{..}</m>. That is, we would use this as our estimate of the
        effect<fn>Again, we switched <m>\mu</m> to <m>\bar{Y}</m> to indicate that we're using the
        sample mean instead of the population mean.</fn>:
        <me>\hat{\alpha}_r = \bar{Y}_{r.} - \bar{Y}_{..}</me>
        Similarly, our estimate of the main effect of Factor B at level <m>c</m> can be defined as
        follows:
        <me>\hat{\beta}_c = \bar{Y}_{.c} - \bar{Y}_{..}</me>
        Now, if you go back to the formulas of the SS values for the two main effects, you'll notice that
        these effect terms are exactly the quantities that we were squaring and summing! So what's the
        analogue of this for interaction terms? The answer to this can be found by first rearranging the
        formula for the group means <m>\mu_{rc}</m> under the alternative hypothesis, so that we get
        this:
        <md>
          <mrow>(\alpha \beta)_{rc} \amp= \mu_{rc} - \mu_{..} - \alpha_r - \beta_c</mrow>
          <mrow>\amp= \mu_{rc} - \mu_{..} - (\mu_{r.} - \mu_{..}) - (\mu_{.c} - \mu_{..})</mrow>
          <mrow>\amp= \mu_{rc} - \mu_{r.} - \mu_{.c} + \mu_{..}</mrow>
        </md>
        So, once again, if we substitute our sample statistics in place of the population means, we get
        the following as our estimate of the interaction effect for group <m>rc</m>, which is
        <me>\hat{(\alpha\beta)}_{rc} = \bar{Y}_{rc} - \bar{Y}_{r.} - \bar{Y}_{.c} + \bar{Y}_{..}</me>
        Now all we have to do is sum all of these estimates across all <m>R</m> levels of Factor A and
        all <m>C</m> levels of Factor B, and we obtain the following formula for the sum of squares
        associated with the interaction as a whole:
        <me>\mbox{SS}_{A:B} = N \sum_{r=1}^R \sum_{c=1}^C \left( \bar{Y}_{rc} - \bar{Y}_{r.} - \bar{Y}_{.c} + \bar{Y}_{..} \right)^2</me>
        where, we multiply by <m>N</m> because there are <m>N</m> observations in each of the groups,
        and we want our SS values to reflect the variation among <em>observations</em> accounted for by
        the interaction, not the variation among groups.
      </p>
      <p>
        Now that we have a formula for calculating <m>\mbox{SS}_{A:B}</m>, it's important to recognise
        that the interaction term is part of the model (of course), so the total sum of squares
        associated with the model, <m>\mbox{SS}_M</m> is now equal to the sum of the three relevant SS
        values, <m>\mbox{SS}_A + \mbox{SS}_B + \mbox{SS}_{A:B}</m>. The residual sum of squares
        <m>\mbox{SS}_R</m> is still defined as the leftover variation, namely
        <m>\mbox{SS}_T - \mbox{SS}_M</m>, but now that we have the interaction term this becomes
        <me>\mbox{SS}_R = \mbox{SS}_T - (\mbox{SS}_A + \mbox{SS}_B + \mbox{SS}_{A:B})</me>
        As a consequence, the residual sum of squares <m>\mbox{SS}_R</m> will be smaller than in our
        original ANOVA that didn't include interactions.
      </p>
      <p>
        Calculating the degrees of freedom for the interaction is, once again, slightly trickier than the
        corresponding calculation for the main effects. To start with, let's think about the ANOVA model
        as a whole. Once we include interaction effects in the model, we're allowing every single group
        has a unique mean, <m>\mu_{rc}</m>. For an <m>R \times C</m> factorial ANOVA, this means that
        there are <m>R \times C</m> quantities of interest in the model, and only the one constraint:
        all of the group means need to average out to the grand mean. So the model as a whole needs to
        have <m>(R \times C) - 1</m> degrees of freedom. But the main effect of Factor A has <m>R-1</m>
        degrees of freedom, and the main effect of Factor B has <m>C-1</m> degrees of freedom. Which
        means that the degrees of freedom associated with the interaction is:
        <md>
          <mrow>{df}_{A:B} \amp= (R \times C - 1) - (R - 1) - (C - 1)</mrow>
          <mrow>\amp= RC - R - C + 1</mrow>
          <mrow>\amp= (R-1)(C-1)</mrow>
        </md>
        which is just the product of the degrees of freedom associated with the row factor and the column
        factor.
      </p>
      <p>
        What about the residual degrees of freedom? Because we've added interaction terms, which absorb
        some degrees of freedom, there are fewer residual degrees of freedom left over. Specifically,
        note that if the model with interaction has a total of <m>(R \times C) - 1</m>, and there are
        <m>N</m> observations in your data set that are constrained to satisfy 1 grand mean, your
        residual degrees of freedom now become <m>N-(R \times C)-1+1</m>, or just <m>N-(R \times C)</m>.
      </p>
      <p>This means that our residual degrees of freedom are for the model with interaction are:</p>
      <ul>
        <li><p><m>N-(R \times C)</m> for the residual degrees of freedom: <m>18 - (3 \times 2) = 12</m></p></li>
        <li><p><m>(R-1)(C-1)</m> for the interaction degrees of freedom: <m>(3-1)(2-1) = 2</m></p></li>
        <li><p><m>R-1</m> for the main effect of Factor A (<c>drug</c>): 3 levels <m>- 1 = 2</m></p></li>
        <li><p><m>C-1</m> for the main effect of Factor B (<c>therapy</c>): 2 levels <m>- 1 = 1</m></p></li>
      </ul>
      <p>
        This changes the <m>F</m>-statistic for all of the effects in the model. Let's look at the
        CogStat results now.
      </p>

      <remark xml:id="cogstat-hyp10">
        <title>CogStat Output: Hypothesis Tests (Factorial ANOVA)</title>
        <p><alert>Hypothesis tests</alert></p>
        <p><em>Testing if the means are the same.</em></p>
        <p>
          At least two grouping variables. Interval variable. <m>\gg</m> Choosing factorial ANOVA.
        </p>
        <p>Result of multi-way ANOVA:</p>
        <p>Main effect of drug: <m>F(2, 12) = 31.71, p \lt .001</m></p>
        <p>Main effect of therapy: <m>F(1, 12) = 8.58, p = .013</m></p>
        <p>Interaction of drug and therapy: <m>F(2, 12) = 2.49, p = .125</m></p>
      </remark>

      <subsection xml:id="subsec-factorial-interpret">
        <title>How to Interpret the Results</title>

        <p>
          There are a couple of very important things to consider when interpreting the results of
          factorial ANOVA. Firstly, there's the same issue that we had with one-way ANOVA, which is that
          if you obtain a significant main effect of <c>drug</c>, it doesn't tell you anything about
          which drugs are different to one another. Knowing that there's a significant interaction
          doesn't tell you anything about what kind of interaction exists. Again, you'll need to run
          additional analyses.
        </p>
        <p>
          Secondly, there's a very peculiar interpretation issue that arises when you obtain a significant
          interaction effect but no corresponding main effect. This happens sometimes. For instance, in
          the crossover interaction shown in <xref ref="fig-interaction"/>, this is exactly what you'd
          find: in this case, neither of the main effects would be significant, but the interaction
          effect would be. This is a difficult situation to interpret, and people often get a bit confused
          about it. The general advice that statisticians like to give in this situation is that you
          shouldn't pay much attention to the main effects when an interaction is present. The reason
          they say this is that, although the tests of the main effects are perfectly valid from a
          mathematical point of view, when there is a significant interaction effect the main effects
          rarely test interesting hypotheses. Recall that the null hypothesis for a main effect is that
          the <em>marginal means</em> are equal to each other, and that a marginal mean is formed by
          averaging across several different groups. But if you have a significant interaction effect,
          then you <em>know</em> that the groups that comprise the marginal mean aren't homogeneous, so
          it's not really obvious why you would even care about those marginal means.
        </p>
        <p>
          Here's what that means. Again, let's stick with a clinical example. Suppose that we had a
          <m>2 \times 2</m> design comparing two different treatments for phobias (e.g., systematic
          desensitisation vs flooding), and two different anxiety-reducing drugs (e.g., Anxifree vs
          Joyzepam). Now suppose what we found was that Anxifree had no effect when desensitisation was
          the treatment, and Joyzepam had no effect when flooding was the treatment. But both were pretty
          effective for the other treatment. This is a classic crossover interaction, and what we'd find
          when running the ANOVA is that there is no main effect of drug, but a significant interaction.
          Now, what does it actually <em>mean</em> to say that there's no main effect? Well, it means
          that, if we average over the two different psychological treatments, then the <em>average</em>
          effect of Anxifree and Joyzepam is the same. But why would anyone care about that? When
          treating someone for phobias, it is never the case that a person can be treated using an
          <q>average</q> of flooding and desensitisation: that doesn't make a lot of sense. You either
          get one or the other. For one treatment, one drug is effective; and for the other treatment,
          the other drug is effective. The interaction is the important thing; the main effect is kind
          of irrelevant.
        </p>
        <p>
          This sort of thing happens a lot: the main effects are tests of marginal means, and when an
          interaction is present we often find ourselves not being terribly interested in marginal means,
          because they imply averaging over things that the interaction tells us shouldn't be averaged!
          Of course, it's not always the case that a main effect is meaningless when an interaction is
          present. Often you can get a big main effect and a very small interaction, in which case you
          can still say things like <q>drug A is generally more effective than drug B</q> (because there
          was a big effect of drug), but you'd need to modify it a bit by adding that <q>the difference
          in effectiveness was different for different psychological treatments</q>. In any case, the
          main point here is that whenever you get a significant interaction you should stop and
          <em>think</em> about what the main effect actually means in this context. Don't automatically
          assume that the main effect is interesting.
        </p>

      </subsection>

    </subsection>

  </section>

  <!-- ============================================================ -->
  <!-- Section 2: Effect size                                       -->
  <!-- ============================================================ -->
  <section xml:id="sec-effectsizefactorialanova">
    <title>Effect Size</title>

    <p>
      The effect size calculations for a factorial ANOVA should be similar to those used in one-way ANOVA
      (<m>\eta^2</m> and <m>\omega^2</m>). However, when doing a factorial ANOVA, there is a second
      measure of effect size that people like to report, known as partial <m>\eta^2</m>. The idea behind
      partial <m>\eta^2</m> (which is sometimes denoted <m>{}_p\eta^2</m> or <m>\eta^2_p</m>) is that,
      when measuring the effect size for a particular term (e.g. the main effect of Factor A), you want
      to deliberately ignore the other effects in the model (e.g. the main effect of Factor B). That is,
      you would pretend that the effect of all these other terms is zero, and then calculate what the
      <m>\eta^2</m> value would have been. This is actually pretty easy to calculate. All you have to do
      is remove the sum of squares associated with the other terms from the denominator. In other words,
      if you want the partial <m>\eta^2</m> for the main effect of Factor A, the denominator is just the
      sum of the SS values for Factor A and the residuals:
      <me>\mbox{partial } \eta^2_A = \frac{\mbox{SS}_{A}}{\mbox{SS}_{A} + \mbox{SS}_{R}}</me>
    </p>
    <p>
      This will always give you a larger number than <m>\eta^2</m>, which the cynic in me suspects
      accounts for the popularity of partial <m>\eta^2</m>. And once again you get a number between 0
      and 1, where 0 represents no effect. However, it's slightly trickier to interpret what a large
      partial <m>\eta^2</m> value means. In particular, you can't actually compare the partial
      <m>\eta^2</m> values across terms! Suppose, for instance, there is no within-groups variability at
      all: if so, <m>\mbox{SS}_R = 0</m>. What that means is that <em>every</em> term has a partial
      <m>\eta^2</m> value of 1. But that doesn't mean that all terms in your model are equally important,
      or indeed that they are equally large. All it means is that all terms in your model have effect
      sizes that are large <em>relative to the residual variation</em>. It is not comparable across
      terms.
    </p>
    <p>
      CogStat currently doesn't provide effect sizes for multi-way ANOVAs. But if you're interested in
      <m>\eta^2</m> and partial <m>\eta^2</m>, read on.
    </p>
    <p>
      First, let's have a look at the effect sizes for the original ANOVA without the interaction term:
    </p>

    <table xml:id="table-effect-no-interaction">
      <title>Effect sizes for the ANOVA without the interaction term</title>
      <tabular halign="center">
        <row header="yes">
          <cell></cell>
          <cell><m>\eta^2</m></cell>
          <cell>Partial <m>\eta^2</m></cell>
        </row>
        <row>
          <cell>Drug</cell>
          <cell>0.713</cell>
          <cell>0.789</cell>
        </row>
        <row>
          <cell>Therapy</cell>
          <cell>0.096</cell>
          <cell>0.336</cell>
        </row>
      </tabular>
    </table>

    <p>
      Looking at the <m>\eta^2</m> values first, we see that <c>drug</c> accounts for 71.3% of the
      variance (i.e. <m>\eta^2 = 0.713</m>) in <c>mood_gain</c>, whereas <c>therapy</c> only accounts
      for 9.6%. This leaves a total of 19.1% of the variation unaccounted for (i.e. the residuals
      constitute 19.1% of the variation in the outcome). Overall, this implies that we have a very
      large effect<fn>Implausibly large: the artificiality of this data set is really starting to
      show!</fn> of <c>drug</c> and a modest effect of <c>therapy</c>.
    </p>
    <p>
      Now let's look at the partial <m>\eta^2</m> values. Because the effect of <c>therapy</c> isn't all
      that large, controlling for it doesn't make much of a difference, so the partial <m>\eta^2</m> for
      <c>drug</c> doesn't increase very much, and we obtain a value of
      <m>{}_p\eta^2 = 0.789</m>. In contrast, because the effect of <c>drug</c> was very large,
      controlling for it makes a big difference, and so when we calculate the partial <m>\eta^2</m> for
      <c>therapy</c> you can see that it rises to <m>{}_p\eta^2 = 0.336</m>. The question that we have
      to ask ourselves is, what does these partial <m>\eta^2</m> values actually <em>mean</em>?
    </p>
    <p>
      The partial <m>\eta^2</m> for the main effect of Factor A is a statement about a hypothetical
      experiment in which <em>only</em> Factor A was being varied. So, even though in <em>this</em>
      experiment we varied both A and B, we can easily imagine an experiment in which only Factor A was
      varied: the partial <m>\eta^2</m> statistic tells you how much of the variance in the outcome
      variable you would expect to see accounted for in that experiment. However, it should be noted
      that this interpretation — like many things associated with main effects — doesn't make a lot of
      sense when there is a large and significant interaction effect.
    </p>
    <p>
      Speaking of interaction effects, here's what we get when we calculate the effect sizes for the
      model that includes the interaction term. As you can see, the <m>\eta^2</m> values for the main
      effects don't change, but the partial <m>\eta^2</m> values do:
    </p>

    <table xml:id="table-effect-with-interaction">
      <title>Effect sizes for the ANOVA with the interaction term</title>
      <tabular halign="center">
        <row header="yes">
          <cell></cell>
          <cell><m>\eta^2</m></cell>
          <cell>Partial <m>\eta^2</m></cell>
        </row>
        <row>
          <cell>Drug</cell>
          <cell>0.713</cell>
          <cell>0.841</cell>
        </row>
        <row>
          <cell>Therapy</cell>
          <cell>0.096</cell>
          <cell>0.417</cell>
        </row>
        <row>
          <cell>Drug <m>\times</m> Therapy</cell>
          <cell>0.056</cell>
          <cell>0.293</cell>
        </row>
      </tabular>
    </table>

  </section>

  <!-- ============================================================ -->
  <!-- Section 3: Estimated group means and confidence intervals    -->
  <!-- ============================================================ -->
  <section xml:id="sec-meansfactorialanova">
    <title>Estimated Group Means and Confidence Intervals</title>

    <p>
      You will find yourself wanting to report estimates of all the group means based on the results of
      your ANOVA, as well as confidence intervals associated with them. If the ANOVA that you have run is
      a <alert>saturated model</alert> (i.e. contains all possible main effects and all possible
      interaction effects), then the estimates of the group means are actually identical to the sample
      means, though the confidence intervals will use a pooled estimate of the standard errors, rather
      than use a separate one for each group.
    </p>
    <p>
      If you look at the <c>Population parameter estimations</c> output from CogStat, you see that there
      are confidence intervals given for each combination of our variables
      (<xref ref="fig-cogstatafci"/>). Estimated mean mood gain for the placebo group with no therapy
      was <m>0.30</m>, with a 95% confidence interval from <m>-0.20</m> to <m>0.80</m>. However, that
      is when you calculate the confidence intervals separately for each group.
    </p>

    <figure xml:id="fig-cogstatafci">
      <caption>Population parameter estimations: estimated group means — reduced model</caption>
      <image source="cogstatafci.png" width="80%">
        <description>CogStat population parameter estimations output for the clinical trial factorial ANOVA</description>
      </image>
    </figure>

    <p>
      In a saturated model, the estimated mean mood gain for the placebo group with no therapy should
      still be <m>0.30</m>, but with a 95% confidence interval from <m>0.006</m> to <m>0.594</m>.
    </p>

    <table xml:id="table-estgroup-means">
      <title>Estimated group means and confidence intervals</title>
      <tabular halign="center">
        <row header="yes">
          <cell>Drug</cell>
          <cell>Therapy</cell>
          <cell>Mean</cell>
          <cell>Std. Error</cell>
          <cell>Lower Bound</cell>
          <cell>Upper Bound</cell>
        </row>
        <row>
          <cell>Placebo</cell>
          <cell>No therapy</cell>
          <cell>0.300</cell>
          <cell>0.135</cell>
          <cell>0.006</cell>
          <cell>0.594</cell>
        </row>
        <row>
          <cell>Placebo</cell>
          <cell>CBT</cell>
          <cell>0.600</cell>
          <cell>0.135</cell>
          <cell>0.306</cell>
          <cell>0.894</cell>
        </row>
        <row>
          <cell>Anxifree</cell>
          <cell>No therapy</cell>
          <cell>0.400</cell>
          <cell>0.135</cell>
          <cell>0.106</cell>
          <cell>0.694</cell>
        </row>
        <row>
          <cell>Anxifree</cell>
          <cell>CBT</cell>
          <cell>1.033</cell>
          <cell>0.135</cell>
          <cell>0.740</cell>
          <cell>1.327</cell>
        </row>
        <row>
          <cell>Joyzepam</cell>
          <cell>No therapy</cell>
          <cell>1.467</cell>
          <cell>0.135</cell>
          <cell>1.173</cell>
          <cell>1.760</cell>
        </row>
        <row>
          <cell>Joyzepam</cell>
          <cell>CBT</cell>
          <cell>1.500</cell>
          <cell>0.135</cell>
          <cell>1.206</cell>
          <cell>1.794</cell>
        </row>
      </tabular>
    </table>

  </section>

  <!-- ============================================================ -->
  <!-- Section 4: Post hoc tests                                    -->
  <!-- ============================================================ -->
  <section xml:id="sec-posthoc2">
    <title>Post Hoc Tests</title>

    <p>
      Let's suppose you've done your ANOVA, and it turns out that you obtained some significant effects.
      Because of the fact that the <m>F</m>-tests are <q>omnibus</q> tests that only really test the
      null hypothesis that there are no differences among groups, obtaining a significant effect doesn't
      tell you which groups are different to which other ones.
    </p>
    <p>
      We discussed this issue in the one-way ANOVA chapter, and in that chapter we mentioned an easy
      solution to run <m>t</m>-tests for all possible pairs of groups, but CogStat opts for Tukey's HSD,
      which is the right call in terms of ANOVA.
    </p>
    <p>We would be interested in the following four comparisons:</p>
    <ul>
      <li><p>The difference in mood gain for people given Anxifree versus people given the placebo.</p></li>
      <li><p>The difference in mood gain for people given Joyzepam versus people given the placebo.</p></li>
      <li><p>The difference in mood gain for people given Anxifree versus people given Joyzepam.</p></li>
      <li><p>The difference in mood gain for people treated with CBT and people given no therapy.</p></li>
    </ul>
    <p>
      For any one of these comparisons, we're interested in the true difference between (population) group
      means. Tukey's HSD constructs <alert>simultaneous confidence intervals</alert> for all four of
      these comparisons. What we mean by 95% <q>simultaneous</q> confidence interval is that there is a
      95% probability that <em>all</em> of these confidence intervals contain the relevant true value.
      Moreover, we can use these confidence intervals to calculate an adjusted <m>p</m> value for any
      specific comparison.
    </p>
    <p>Currently, CogStat does not do a stacked Tukey's HSD for multi-way ANOVA.</p>
    <p>
      We can run the <c>Compare groups</c> function in CogStat but with only <c>drug</c> as grouping
      variable, as we did in the one-way ANOVA chapter. The confidence intervals would be slightly
      different, though. For the second variable, namely <c>therapy</c>, we don't get a Tukey's HSD, as
      it's a two-level factor.
    </p>
    <p>
      But what would be quite interesting, is the situation where your model includes interaction terms.
      The number of pairwise comparisons that we would need to consider starts to increase. As before, we
      need to consider the three comparisons that are relevant to the main effect of <c>drug</c> and the
      one comparison that is relevant to the main effect of <c>therapy</c>. But, if we want to consider
      the possibility of a significant interaction (and try to find the group differences that underpin
      that significant interaction), we need to include comparisons such as the following:
    </p>
    <ul>
      <li>
        <p>
          The difference in mood gain for people given Anxifree and treated with CBT, versus people given
          the placebo and treated with CBT
        </p>
      </li>
      <li>
        <p>
          The difference in mood gain for people given Anxifree and given no therapy, versus people given
          the placebo and given no therapy.
        </p>
      </li>
      <li><p>etc.</p></li>
    </ul>
    <p>
      There are quite a lot of these comparisons that you need to consider. So, the ideal output of
      Tukey's HSD would make a <em>lot</em> of pairwise comparisons (19 in total). This is not yet
      possible in CogStat, but let's look at what the result would look like if it were.
    </p>

    <table xml:id="table-tukey-hsd">
      <title>Tukey's HSD for a multi-way ANOVA</title>
      <tabular halign="center">
        <row header="yes">
          <cell>Pair</cell>
          <cell>Difference</cell>
          <cell>CI 95% (Lower)</cell>
          <cell>CI 95% (Upper)</cell>
          <cell><m>p</m></cell>
        </row>
        <row>
          <cell colspan="5"><em>… - placebo-no_therapy</em></cell>
        </row>
        <row>
          <cell>anxifree-no_therapy</cell>
          <cell>0.100</cell>
          <cell>-0.540</cell>
          <cell>0.740</cell>
          <cell>0.994</cell>
        </row>
        <row>
          <cell>joyzepam-no_therapy</cell>
          <cell>1.167</cell>
          <cell>0.527</cell>
          <cell>1.807</cell>
          <cell>0.001</cell>
        </row>
        <row>
          <cell>placebo-CBT</cell>
          <cell>0.300</cell>
          <cell>-0.340</cell>
          <cell>0.940</cell>
          <cell>0.628</cell>
        </row>
        <row>
          <cell>anxifree-CBT</cell>
          <cell>0.733</cell>
          <cell>0.093</cell>
          <cell>1.373</cell>
          <cell>0.022</cell>
        </row>
        <row>
          <cell>joyzepam-CBT</cell>
          <cell>1.200</cell>
          <cell>0.560</cell>
          <cell>1.840</cell>
          <cell>0.000</cell>
        </row>
        <row>
          <cell colspan="5"><em>… - anxifree-no_therapy</em></cell>
        </row>
        <row>
          <cell>joyzepam-no_therapy</cell>
          <cell>1.067</cell>
          <cell>0.427</cell>
          <cell>1.707</cell>
          <cell>0.001</cell>
        </row>
        <row>
          <cell>placebo-CBT</cell>
          <cell>0.200</cell>
          <cell>-0.440</cell>
          <cell>0.840</cell>
          <cell>0.892</cell>
        </row>
        <row>
          <cell>anxifree-CBT</cell>
          <cell>0.633</cell>
          <cell>-0.007</cell>
          <cell>1.273</cell>
          <cell>0.053</cell>
        </row>
        <row>
          <cell>joyzepam-CBT</cell>
          <cell>1.100</cell>
          <cell>0.460</cell>
          <cell>1.740</cell>
          <cell>0.001</cell>
        </row>
        <row>
          <cell colspan="5"><em>… - joyzepam-no_therapy</em></cell>
        </row>
        <row>
          <cell>placebo-CBT</cell>
          <cell>-0.867</cell>
          <cell>-1.507</cell>
          <cell>-0.227</cell>
          <cell>0.007</cell>
        </row>
        <row>
          <cell>anxifree-CBT</cell>
          <cell>-0.433</cell>
          <cell>-1.073</cell>
          <cell>0.207</cell>
          <cell>0.275</cell>
        </row>
        <row>
          <cell>joyzepam-CBT</cell>
          <cell>0.033</cell>
          <cell>-0.607</cell>
          <cell>0.673</cell>
          <cell>1.000</cell>
        </row>
        <row>
          <cell colspan="5"><em>… - placebo-CBT</em></cell>
        </row>
        <row>
          <cell>anxifree-CBT</cell>
          <cell>0.433</cell>
          <cell>-0.207</cell>
          <cell>1.073</cell>
          <cell>0.275</cell>
        </row>
        <row>
          <cell>joyzepam-CBT</cell>
          <cell>0.900</cell>
          <cell>0.260</cell>
          <cell>1.540</cell>
          <cell>0.005</cell>
        </row>
        <row>
          <cell colspan="5"><em>… - anxifree-CBT</em></cell>
        </row>
        <row>
          <cell>joyzepam-CBT</cell>
          <cell>0.467</cell>
          <cell>-0.173</cell>
          <cell>1.107</cell>
          <cell>0.214</cell>
        </row>
      </tabular>
    </table>

  </section>

  <!-- ============================================================ -->
  <!-- Section 5: Unbalanced designs and types of sums of squares   -->
  <!-- ============================================================ -->
  <section xml:id="sec-unbalancedanova">
    <title>Unbalanced Designs and Types of Sums of Squares</title>

    <p>
      In real life, we're rarely lucky enough to have perfectly balanced designs. For one reason or
      another, it's typical to end up with more observations in some cells than in others. Or, to put it
      another way, we have an <alert>unbalanced design</alert>.
    </p>
    <p>
      Unbalanced designs need to be treated with a lot more care than balanced designs, and the
      statistical theory that underpins them is a lot messier. But we must not ignore this issue, like a
      lot of stats textbooks (or psychology students) tend to do. The net result of this is that a lot
      of active researchers in the field don't actually know that there are several different <q>types</q>
      of unbalanced ANOVAs, and they produce quite different answers. In fact, reading the psychological
      literature, it seems that most people don't even realise that their statistical software package is
      making a whole lot of substantive data analysis decisions on their behalf. CogStat will also make
      all decisions for you, but when interpreting the results, it's important to know what the software
      is doing and what is the underlying statistical theory.
    </p>
    <p>
      As usual, it will help us to work with some data. The <c>coffee.csv</c> file contains a
      hypothetical data set that produces an unbalanced <m>3 \times 2</m> ANOVA. Suppose we were
      interested in finding out whether or not the tendency of people to <c>babble</c> when they have
      too much coffee is purely an effect of the coffee itself, or whether there's some effect of the
      <c>milk</c> and <c>sugar</c> that people add to the coffee. Suppose we took 18 people, and gave
      them some coffee to drink. The amount of coffee / caffeine was held constant, and we varied whether
      or not milk was added: so <c>milk</c> is a binary factor with two levels, <c>"yes"</c> and
      <c>"no"</c>. We also varied the kind of sugar involved. The coffee might contain <c>"real"</c>
      sugar, or it might contain <c>"fake"</c> sugar (i.e., artificial sweetener), or it might contain
      <c>"none"</c> at all, so the <c>sugar</c> variable is a three-level factor. Our outcome variable
      is a continuous variable that presumably refers to some psychologically sensible measure of the
      extent to which someone is <q>babbling</q>. The details don't really matter for our purpose. To
      get a sense of what the data look like, let's load it into CogStat and run the
      <c>Compare groups</c> function. Let's look at the sample properties:
    </p>

    <figure xml:id="fig-cogstatcoffeedescr">
      <caption>The coffee data set loaded in CogStat</caption>
      <image source="cogstatcoffeedescr.png" width="80%">
        <description>CogStat showing descriptive statistics for the coffee data set</description>
      </image>
    </figure>

    <p>
      Across groups, the standard deviation varies from <m>.10</m> to <m>.62</m>, which is fairly small
      relative to the differences in group means. So far, it's looking like a straightforward factorial
      ANOVA, just like we did earlier.
    </p>
    <p>
      Unbalanced designs lead us to the somewhat unsettling discovery that there isn't really any one
      thing that we might refer to as a standard ANOVA. In fact, it turns out that there are
      <em>three</em> fundamentally different ways<fn>Actually, this is a bit of a lie. ANOVAs can vary
      in other ways besides the ones discussed in this book. For instance, we ignored the difference
      between fixed-effect models, in which the levels of a factor are <q>fixed</q> by the experimenter
      or the world, and random-effect models, in which the levels are random samples from a larger
      population of possible levels (this book only covers fixed-effect models). Don't make the mistake
      of thinking that this book — or any other one — will tell you <q>everything you need to know</q>
      about statistics, any more than a single book could possibly tell you everything you need to know
      about psychology, physics or philosophy. Life is too complicated for that to <em>ever</em> be true.
      This isn't a cause for despair, though. Most researchers get by with a basic working knowledge of
      ANOVA that doesn't go any further than this book does. Keep in mind that this book is only the
      beginning of a very long story, not the whole story.</fn> in which you might want to run an ANOVA
      in an unbalanced design. If you have a balanced design, all three versions produce identical
      results, with the sums of squares, <m>F</m>-values and so on. All conforming to the formulas from
      the start of this chapter. However, when your design is unbalanced they don't give the same
      answers. Furthermore, they are not all equally appropriate to every situation: some methods will
      be more appropriate to your situation than others. Given all this, it's important to understand
      what the different types of ANOVA are and how they differ from one another.
    </p>
    <p>
      The first kind of ANOVA is conventionally referred to as <em>Type I sum of squares</em>. You can
      guess what the other two are called. The <q>sum of squares</q> part of the name was introduced by
      the SAS statistical software package, and has become standard nomenclature, but it's a bit
      misleading in some ways. The logic for referring to them as different types of sum of squares is
      that, when you look at the ANOVA tables that they produce, the key difference in the numbers is
      the SS values. The degrees of freedom don't change, the MS values are still defined as SS divided
      by df, etc. However, what the terminology gets wrong is that it hides the reason <em>why</em> the
      SS values are different from one another.
    </p>
    <p>
      To that end, it's a lot more helpful to think of the three different kinds of ANOVA as three
      different <em>hypothesis testing strategies</em>. These different strategies lead to different SS
      values, to be sure, but it's the strategy that is the important thing here, not the SS values
      themselves. Any particular <m>F</m>-test is best thought of as a comparison between two linear
      models. So when you're looking at an ANOVA result, it helps to remember that each of those
      <m>F</m>-tests corresponds to a <em>pair</em> of models that are being compared. Of course, this
      leads naturally to the question of <em>which</em> pair of models is being compared. This is the
      fundamental difference between ANOVA Types I, II and III: each one corresponds to a different way
      of choosing the model pairs for the tests.
    </p>

    <subsection xml:id="subsec-type1-ss">
      <title>Type I Sum of Squares</title>

      <p>
        The Type I method is sometimes referred to as the <q>sequential</q> sum of squares, because it
        involves a process of adding terms to the model one at a time. Consider the coffee data, for
        instance. Suppose we want to run the full <m>3 \times 2</m> factorial ANOVA, including
        interaction terms.
      </p>
      <p>
        The simplest possible model for the data would be one in which neither milk nor sugar is assumed
        to have any effect on babbling. The only term that would be included in such a model is the
        intercept. This is our initial null hypothesis.
      </p>
      <p>
        The next simplest model for the data would be one in which only one of the two main effects is
        included. In the coffee data, there are two different possible choices here, because we could
        choose to add milk first or to add sugar first. The order actually matters, but for now, let's
        just make a choice arbitrarily, and pick sugar. So the second model in our sequence of models is
        <c>babble | sugar</c>. This comparison forms our hypothesis test of the main effect of
        <c>sugar</c>. The next step in our model-building exercise it to add the other main effect term,
        so the next model in our sequence is <c>babble | sugar + milk</c>. This comparison forms our
        hypothesis test of the main effect of <c>milk</c>. In one sense, this approach is very elegant:
        the alternative hypothesis from the first test forms the null hypothesis for the second one. It
        is in this sense that the Type I method is strictly sequential. Every test builds directly on
        the results of the last one. However, in another sense, it's very inelegant because there's a
        strong asymmetry between the two tests. The test of the main effect of <c>sugar</c> (the first
        test) completely ignores <c>milk</c>, whereas the test of the main effect of <c>milk</c> (the
        second test) does take <c>sugar</c> into account. In any case, the fourth model in our sequence
        is now the full model, <c>babble | sugar + milk + sugar:milk</c>.
      </p>

      <table xml:id="table-type1-sugar-first">
        <title>Hypotheses for the Type I Sum of Squares for the <c>coffee</c> dataset (sugar first)</title>
        <tabular>
          <row header="yes">
            <cell>Model</cell>
            <cell><m>H_0</m> Null hypothesis</cell>
            <cell><m>H_1</m> Alternative hypothesis</cell>
            <cell>Main effect</cell>
          </row>
          <row>
            <cell>Model 1</cell>
            <cell><c>babble | 1</c></cell>
            <cell><c>babble | sugar</c></cell>
            <cell>sugar</cell>
          </row>
          <row>
            <cell>Model 2</cell>
            <cell><c>babble | sugar</c></cell>
            <cell><c>babble | sugar + milk</c></cell>
            <cell>milk</cell>
          </row>
          <row>
            <cell>Model 3</cell>
            <cell><c>babble | sugar + milk</c></cell>
            <cell><c>babble | sugar + milk + sugar×milk</c></cell>
            <cell>sugar×milk</cell>
          </row>
        </tabular>
      </table>

      <p>
        When run (in another statistical software package that allows for selection of Type I, II or III),
        the results of the analysis are the following:
      </p>

      <table xml:id="table-type1-sugar-first-results">
        <title>Test of between-subject effects with Type I Sum of Squares for the <c>coffee</c> dataset (sugar first)</title>
        <tabular halign="center">
          <row header="yes">
            <cell>Source</cell>
            <cell>Type I SS</cell>
            <cell>df</cell>
            <cell>MS</cell>
            <cell><m>F</m></cell>
            <cell><m>p</m></cell>
          </row>
          <row>
            <cell>sugar</cell>
            <cell>3.558</cell>
            <cell>2</cell>
            <cell>1.779</cell>
            <cell>6.749</cell>
            <cell>0.011</cell>
          </row>
          <row>
            <cell>milk</cell>
            <cell>0.956</cell>
            <cell>1</cell>
            <cell>0.956</cell>
            <cell>3.628</cell>
            <cell>0.081</cell>
          </row>
          <row>
            <cell>sugar×milk</cell>
            <cell>5.944</cell>
            <cell>2</cell>
            <cell>2.972</cell>
            <cell>11.277</cell>
            <cell>0.002</cell>
          </row>
        </tabular>
      </table>

      <p>
        The big problem with using Type I sum of squares is the fact that it really does depend on the
        order in which you enter the variables. Yet, in many situations, the researcher has no reason to
        prefer one ordering over another. This is presumably the case for our milk and sugar problem.
        Should we add milk first, or sugar first? It feels exactly as arbitrary as a data analysis
        question as it does as a coffee-making question. There may in fact be some people with firm
        opinions about ordering, but it's hard to imagine a principled answer to the question. Yet, look
        what happens when we change the ordering:
      </p>

      <table xml:id="table-type1-milk-first">
        <title>Hypotheses for the Type I Sum of Squares for the <c>coffee</c> dataset (milk first)</title>
        <tabular>
          <row header="yes">
            <cell>Model</cell>
            <cell><m>H_0</m> Null hypothesis</cell>
            <cell><m>H_1</m> Alternative hypothesis</cell>
            <cell>Main effect</cell>
          </row>
          <row>
            <cell>Model 1</cell>
            <cell><c>babble | 1</c></cell>
            <cell><c>babble | milk</c></cell>
            <cell>milk</cell>
          </row>
          <row>
            <cell>Model 2</cell>
            <cell><c>babble | milk</c></cell>
            <cell><c>babble | milk + sugar</c></cell>
            <cell>sugar</cell>
          </row>
          <row>
            <cell>Model 3</cell>
            <cell><c>babble | milk + sugar</c></cell>
            <cell><c>babble | milk + sugar + milk×sugar</c></cell>
            <cell>milk×sugar</cell>
          </row>
        </tabular>
      </table>

      <table xml:id="table-type1-milk-first-results">
        <title>Test of between-subject effects with Type I Sum of Squares for the <c>coffee</c> dataset (milk first)</title>
        <tabular halign="center">
          <row header="yes">
            <cell>Source</cell>
            <cell>Type I SS</cell>
            <cell>df</cell>
            <cell>MS</cell>
            <cell><m>F</m></cell>
            <cell><m>p</m></cell>
          </row>
          <row>
            <cell>milk</cell>
            <cell>1.444</cell>
            <cell>1</cell>
            <cell>1.444</cell>
            <cell>5.479</cell>
            <cell>0.037</cell>
          </row>
          <row>
            <cell>sugar</cell>
            <cell>3.070</cell>
            <cell>2</cell>
            <cell>1.535</cell>
            <cell>5.824</cell>
            <cell>0.017</cell>
          </row>
          <row>
            <cell>milk×sugar</cell>
            <cell>5.944</cell>
            <cell>2</cell>
            <cell>2.972</cell>
            <cell>11.277</cell>
            <cell>0.002</cell>
          </row>
        </tabular>
      </table>

      <p>
        The <m>p</m>-values for both main effect terms have changed, and fairly dramatically. Among
        other things, the effect of <c>milk</c> has become significant (though one should avoid drawing
        any strong conclusions about this, as I've mentioned previously). Which of these two ANOVAs
        should one report? It's not immediately obvious.
      </p>
      <p>
        When you look at the hypothesis tests that are used to define the <q>first</q> main effect and
        the <q>second</q> one, it's clear that they're qualitatively different from one another. In our
        initial example, we saw that the test for the main effect of <c>sugar</c> completely ignores
        <c>milk</c>, whereas the test of the main effect of <c>milk</c> does take <c>sugar</c> into
        account. As such, the Type I testing strategy really does treat the first main effect as if it
        had a kind of theoretical primacy over the second one. There is very rarely if ever any
        theoretically primacy of this kind that would justify treating any two main effects asymmetrically.
      </p>
      <p>
        The consequence of all this is that Type I tests are very rarely of much interest.
      </p>

    </subsection>

    <subsection xml:id="subsec-type3-ss">
      <title>Type III Sum of Squares</title>

      <p>
        Having just finished talking about Type I tests, you might think that the natural thing to do
        next would be to talk about Type II tests. However, it's actually a bit more natural to discuss
        Type III tests (which are simple) before talking about Type II tests (which are trickier). The
        basic idea behind Type III tests is extremely simple: regardless of which term you're trying to
        evaluate, run the <m>F</m>-test in which the alternative hypothesis corresponds to the full ANOVA
        model as specified by the user, and the null model just deletes that one term that you're testing.
        For instance, in the coffee example, the hypotheses would look like this:
      </p>

      <table xml:id="table-type3-hypotheses">
        <title>Hypotheses for the Type III Sum of Squares for the <c>coffee</c> dataset</title>
        <tabular>
          <row header="yes">
            <cell>Model</cell>
            <cell><m>H_0</m> Null hypothesis</cell>
            <cell><m>H_1</m> Alternative hypothesis</cell>
            <cell>Main effect</cell>
          </row>
          <row>
            <cell>Model 1</cell>
            <cell><c>babble | milk + sugar×milk</c></cell>
            <cell><c>babble | sugar + milk + sugar×milk</c></cell>
            <cell>sugar</cell>
          </row>
          <row>
            <cell>Model 2</cell>
            <cell><c>babble | sugar + sugar×milk</c></cell>
            <cell><c>babble | sugar + milk + sugar×milk</c></cell>
            <cell>milk</cell>
          </row>
          <row>
            <cell>Model 3</cell>
            <cell><c>babble | sugar + milk</c></cell>
            <cell><c>babble | sugar + milk + sugar×milk</c></cell>
            <cell>sugar×milk</cell>
          </row>
        </tabular>
      </table>

      <p>
        At first pass, Type III tests seem like a nice idea. Firstly, we've removed the asymmetry that
        caused us to have problems when running Type I tests. And because we're now treating all terms
        the same way, the results of the hypothesis tests do not depend on the order in which we specify
        them. This is definitely a good thing.
      </p>
      <p>
        However, there is a big problem when interpreting the results of the tests, especially for main
        effect terms. Consider the coffee data. Suppose it turns out that the main effect of <c>milk</c>
        is not significant according to the Type III tests. What this is telling us is that
        <c>babble | sugar + sugar×milk</c> is a better model for the data than the full model. But what
        does that even <em>mean</em>? If the interaction term <c>sugar×milk</c> was also non-significant,
        we'd be tempted to conclude that the data are telling us that the only thing that matters is
        <c>sugar</c>. But suppose we have a significant interaction term, but a non-significant main
        effect of <c>milk</c>. In this case, are we to assume that there really is an <q>effect of
        sugar</q>, an <q>interaction between milk and sugar</q>, but no <q>effect of milk</q>? That
        seems crazy. The right answer simply <em>must</em> be that it's
        meaningless<fn>Or, at the very least, rarely of interest.</fn> to talk about the main effect if
        the interaction is significant. In general, this seems to be what most statisticians advise us
        to do. But if it really is meaningless to talk about non-significant main effects in the presence
        of a significant interaction, then it's not at all obvious why Type III tests should allow the
        null hypothesis to rely on a model that includes the interaction but omits one of the main
        effects that make it up. When characterised in this fashion, the null hypotheses really don't
        make much sense at all.
      </p>
      <p>Let's look at the results:</p>

      <table xml:id="table-type3-results">
        <title>Test of between-subject effects with Type III Sum of Squares for the <c>coffee</c> dataset</title>
        <tabular halign="center">
          <row header="yes">
            <cell>Source</cell>
            <cell>Type III SS</cell>
            <cell>df</cell>
            <cell>MS</cell>
            <cell><m>F</m></cell>
            <cell><m>p</m></cell>
          </row>
          <row>
            <cell>milk</cell>
            <cell>1.004</cell>
            <cell>1</cell>
            <cell>1.004</cell>
            <cell>3.810</cell>
            <cell>0.075</cell>
          </row>
          <row>
            <cell>sugar</cell>
            <cell>2.132</cell>
            <cell>2</cell>
            <cell>1.066</cell>
            <cell>4.045</cell>
            <cell>0.045</cell>
          </row>
          <row>
            <cell>milk×sugar</cell>
            <cell>5.944</cell>
            <cell>2</cell>
            <cell>2.972</cell>
            <cell>11.277</cell>
            <cell>0.002</cell>
          </row>
        </tabular>
      </table>

      <p>
        You'll notice that CogStat will automatically use Type III Sums of Squares. This is exactly the
        same result set that we get from CogStat whether we put <c>milk</c> or <c>sugar</c> first in the
        <c>Compare groups</c> dialog's <c>Group(s)</c> list.
      </p>
      <p>With <c>milk</c> first, we get:</p>

      <remark xml:id="cogstat-hyp11">
        <title>CogStat Output: Hypothesis Tests (coffee, milk first)</title>
        <p><alert>Hypothesis tests</alert></p>
        <p><em>Testing if the means are the same.</em></p>
        <p>
          At least two grouping variables. Interval variable. <m>\gg</m> Choosing factorial ANOVA.
        </p>
        <p>Result of multi-way ANOVA:</p>
        <p>Main effect of milk: <m>F(1, 12) = 3.81, p = .075</m></p>
        <p>Main effect of sugar: <m>F(2, 12) = 4.04, p = .045</m></p>
        <p>Interaction of milk and sugar: <m>F(2, 12) = 11.28, p = .002</m></p>
      </remark>

      <p>With <c>sugar</c> first, we get:</p>

      <remark xml:id="cogstat-hyp12">
        <title>CogStat Output: Hypothesis Tests (coffee, sugar first)</title>
        <p><alert>Hypothesis tests</alert></p>
        <p><em>Testing if the means are the same.</em></p>
        <p>
          At least two grouping variables. Interval variable. <m>\gg</m> Choosing factorial ANOVA.
        </p>
        <p>Result of multi-way ANOVA:</p>
        <p>Main effect of sugar: <m>F(2, 12) = 4.04, p = .045</m></p>
        <p>Main effect of milk: <m>F(1, 12) = 3.81, p = .075</m></p>
        <p>Interaction of sugar and milk: <m>F(2, 12) = 11.28, p = .002</m></p>
      </remark>

    </subsection>

    <subsection xml:id="subsec-type2-ss">
      <title>Type II Sum of Squares</title>

      <p>
        Type II tests are broadly similar to Type III tests: start with a <q>full</q> model, and test a
        particular term by deleting it from that model. However, Type II tests are based on the
        <alert>marginality principle</alert> which states that you should not omit a lower order term
        from your model if there are any higher order ones that depend on it. So, for instance, if your
        model contains the interaction <c>A:B</c> (a 2nd order term), then it really ought to contain
        the main effects <c>A</c> and <c>B</c> (1st order terms). Similarly, if it contains a three-way
        interaction term <c>A:B:C</c>, then the model must also include the main effects <c>A</c>,
        <c>B</c> and <c>C</c> as well as the simpler interactions <c>A:B</c>, <c>A:C</c> and
        <c>B:C</c>. Type III tests routinely violate the marginality principle. For instance, consider
        the test of the main effect of <c>A</c> in the context of a three-way ANOVA that includes all
        possible interaction terms. According to Type III tests, our null and alternative models are:
      </p>

      <table xml:id="table-type3-null-alt">
        <title>Type III null and alternative models for the main effect of <c>A</c></title>
        <tabular>
          <row>
            <cell>Null model:</cell>
            <cell><c>outcome | B + C + A:B + A:C + B:C + A:B:C</c></cell>
          </row>
          <row>
            <cell>Alternative model:</cell>
            <cell><c>outcome | A + B + C + A:B + A:C + B:C + A:B:C</c></cell>
          </row>
        </tabular>
      </table>

      <p>
        Notice that the null hypothesis omits <c>A</c>, but includes <c>A:B</c>, <c>A:C</c> and
        <c>A:B:C</c> as part of the model. This, according to the Type II tests, is not a good choice
        of null hypothesis. What we should do instead, if we want to test the null hypothesis that
        <c>A</c> is not relevant to our <c>outcome</c>, is to specify the null hypothesis that is the
        most complicated model that does not rely on <c>A</c> in any form, even as an interaction. The
        alternative hypothesis corresponds to this null model plus a main effect term of <c>A</c>. This
        is a lot closer to what most people would intuitively think of as a <q>main effect of
        <c>A</c></q>, and it yields the following as our Type II test of the main effect of
        <c>A</c>.<fn>Note, of course, that this does depend on the model that the user specified. If
        original ANOVA model doesn't contain an interaction term for <c>B:C</c>, then obviously it won't
        appear in either the null or the alternative. But that's true for Types I, II and III. They
        never include any terms that you <em>didn't</em> include, but they make different choices about
        how to construct tests for the ones that you did include.</fn>
      </p>

      <table xml:id="table-type2-null-alt">
        <title>Type II null and alternative models for the main effect of <c>A</c></title>
        <tabular>
          <row>
            <cell>Null model:</cell>
            <cell><c>outcome | B + C + B:C</c></cell>
          </row>
          <row>
            <cell>Alternative model:</cell>
            <cell><c>outcome | A + B + C + B:C</c></cell>
          </row>
        </tabular>
      </table>

      <p>
        In the context of the two way ANOVA that we've been using in the coffee data, the hypothesis
        tests are even simpler.
      </p>

      <table xml:id="table-type2-hypotheses">
        <title>Hypotheses for the Type II Sum of Squares for the <c>coffee</c> dataset</title>
        <tabular>
          <row header="yes">
            <cell>Model</cell>
            <cell><m>H_0</m> Null hypothesis</cell>
            <cell><m>H_1</m> Alternative hypothesis</cell>
            <cell>Main effect</cell>
          </row>
          <row>
            <cell>Model 1</cell>
            <cell><c>babble | milk</c></cell>
            <cell><c>babble | sugar + milk</c></cell>
            <cell>sugar</cell>
          </row>
          <row>
            <cell>Model 2</cell>
            <cell><c>babble | sugar</c></cell>
            <cell><c>babble | sugar + milk</c></cell>
            <cell>milk</cell>
          </row>
          <row>
            <cell>Model 3</cell>
            <cell><c>babble | sugar + milk</c></cell>
            <cell><c>babble | sugar + milk + sugar×milk</c></cell>
            <cell>sugar×milk</cell>
          </row>
        </tabular>
      </table>

      <p>The results would be:</p>

      <table xml:id="table-type2-results">
        <title>Test of between-subject effects with Type II Sum of Squares for the <c>coffee</c> dataset</title>
        <tabular halign="center">
          <row header="yes">
            <cell>Source</cell>
            <cell>Type II SS</cell>
            <cell>df</cell>
            <cell>MS</cell>
            <cell><m>F</m></cell>
            <cell><m>p</m></cell>
          </row>
          <row>
            <cell>milk</cell>
            <cell>0.956</cell>
            <cell>1</cell>
            <cell>0.956</cell>
            <cell>3.628</cell>
            <cell>0.081</cell>
          </row>
          <row>
            <cell>sugar</cell>
            <cell>3.070</cell>
            <cell>2</cell>
            <cell>1.535</cell>
            <cell>5.824</cell>
            <cell>0.017</cell>
          </row>
          <row>
            <cell>milk×sugar</cell>
            <cell>5.944</cell>
            <cell>2</cell>
            <cell>2.972</cell>
            <cell>11.277</cell>
            <cell>0.002</cell>
          </row>
        </tabular>
      </table>

      <p>
        Type II tests have some clear advantages over Type I and Type III tests: they don't depend on
        the order in which you specify factors (unlike Type I), and they don't depend on the
        contrasts<fn>Which is something we might discuss in a future version of this book.</fn> that you
        use to specify your factors (unlike Type III).
      </p>
      <p>
        For the moment, you only have Type III available in CogStat, and it is also the default setting
        in SPSS. It is important, though, that in psychological literature, researchers seldom bother to
        report which Type of tests they ran. Often they don't report what software they used either.
      </p>
      <p>
        Make sure you indicate what software you used, and if you're reporting ANOVA results for
        unbalanced data, then specify what type of tests you ran. Or, even better, do hypotheses tests
        that correspond to things you really care about, and then report those!
      </p>

    </subsection>

  </section>

</chapter>
